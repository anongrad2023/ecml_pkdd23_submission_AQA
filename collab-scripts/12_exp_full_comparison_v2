{"cells":[{"cell_type":"markdown","metadata":{"id":"E2UNaoi3uMlP"},"source":["# Overview\n"]},{"cell_type":"markdown","metadata":{"id":"RNJC4tXOuONp"},"source":["In this script, we look at all 4 views of the data, but using a Multi-Label approach for Content-Only and Content-Context views, and using the binary classification model for the Content-Question and Content-Context-Question views of the dataset. \n","\n","This requires 2 sets of data, 2 sets of dataloaders, and 2 sets of models. One for the Multi-Label models/views, and one for the Binary views.\n","\n","This will be a good chance to clean up the script structure. "]},{"cell_type":"markdown","metadata":{"id":"AbeAB8Hokb-2"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"0fSMAjHpkb-4"},"source":["## Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNU4HLNZkb-5"},"outputs":[],"source":["RESULTS_GCS_DIR = \"results/AQA12\"\n","\n","GCS_APP_ID = \"aqa-research\"\n","GCS_BUCKET = \"dabi-aqa-data-00\"\n","\n","FN_QUESTIONS = \"questions_01.csv\"\n","FN_CONTEXTS  = \"contexts_01.csv\"\n","\n","FN_A2_RAW = \"a2_raw.csv\"\n","FN_A2_FA  = \"a2_gs.csv\"\n","FN_A3_FA  = \"a3_gs.csv\"\n","FN_A3_MAJ = \"a3_con.csv\"\n","\n","MAX_SEQ_LEN = 128\n","\n","EPOCHS = 25\n","BATCH_SIZE = 16\n","TEST_FRAC = 0.1\n","\n","RAND_SEED = 4246"]},{"cell_type":"markdown","metadata":{"id":"FC4VN9OX7ASA"},"source":["## Pippin'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5335,"status":"ok","timestamp":1680283256707,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"MjPN6S5z6_7_","outputId":"ea0e85fd-6557-4e2d-c9a7-a29de382a926"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.4\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9945,"status":"ok","timestamp":1680283267974,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"F0uhaIcV7LtL","outputId":"2699384f-5412-4da1-c531-985af4bcc0a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"ybu8iEZskb-3"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiAEt7P6kb-3"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import torch\n","import random\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from tqdm.notebook import tqdm\n","import pickle\n","\n","import transformers as ppb\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","\n","from torch.optim import SGD\n","from torchmetrics.classification import MultilabelAccuracy"]},{"cell_type":"markdown","metadata":{"id":"glKP3nrhkb-6"},"source":["## GCS Auth - Input Needed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13088,"status":"ok","timestamp":1680283286511,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"xwxl9-b4kb-7","outputId":"0fcce899-2707-424b-a4e0-a5801bc5dec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Go to the following link in your browser:\n","\n","    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=XO2yclr8JHL16JN9BwzDpmNkUFALJb&prompt=consent&access_type=offline&code_challenge=-gkobM1QZWiMT3_UuUH1yBGLpKWCiw87DEzlskd3WII&code_challenge_method=S256\n","\n","Enter authorization code: 4/0AVHEtk520HGlpqff5Ql8eQs7LYRJuilkZ5-kNxxo6BMNnyC5Sy63xy1GDHryAaqJ_MhXEw\n","\n","You are now logged in as [willpowe@gmail.com].\n","Your current project is [None].  You can change this setting by running:\n","  $ gcloud config set project PROJECT_ID\n"]}],"source":["!gcloud auth login --launch-browser"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1042,"status":"ok","timestamp":1680283287545,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"SEQdfjlikb-7","outputId":"9bb62a1e-b56b-46dc-ee78-4efe5250d02b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n"]}],"source":["!gcloud config set project {GCS_APP_ID}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x_hDs2gkb-8"},"outputs":[],"source":["def download_file_from_gcs(src_fn, dest_fn):\n","  dest = f\"/content/{dest_fn}\"\n","  dl_command = f\"gsutil -m cp gs://{GCS_BUCKET}/{src_fn} {dest}\"\n","  os.system(dl_command)\n","\n","def upload_file_to_gcs(src_fn, dest_fn):\n","  dest_url = \"{}/{}\".format(GCS_BUCKET, dest_fn)\n","  ul_command = \"gsutil -m cp {} gs://{}\".format(src_fn, dest_url)\n","  os.system(ul_command)"]},{"cell_type":"markdown","metadata":{"id":"TBwBOc1LJWSS"},"source":["## Random Seeds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1680283294529,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"HNrkkYVqJV62","outputId":"57571cf0-67fb-46e2-e134-6876d36bea5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7a2238d7d0>"]},"metadata":{},"execution_count":9}],"source":["random.seed(RAND_SEED)\n","np.random.seed(RAND_SEED)\n","torch.manual_seed(RAND_SEED)"]},{"cell_type":"markdown","metadata":{"id":"YEniTxLUPL-K"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"Df9iIZyhkb--"},"source":["## Raw Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOIG4PnXkb-_"},"outputs":[],"source":["download_file_from_gcs(FN_QUESTIONS, FN_QUESTIONS)\n","download_file_from_gcs(FN_CONTEXTS, FN_CONTEXTS)\n","\n","download_file_from_gcs(FN_A2_RAW, FN_A2_RAW)\n","download_file_from_gcs(FN_A2_FA, FN_A2_FA)\n","download_file_from_gcs(FN_A3_FA, FN_A3_FA)\n","download_file_from_gcs(FN_A3_MAJ, FN_A3_MAJ)"]},{"cell_type":"markdown","metadata":{"id":"CnejujZg8M8q"},"source":["## Dataframes"]},{"cell_type":"markdown","metadata":{"id":"pxorHwUB8Ro-"},"source":["### Question/Context Dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PryV0dbr8SOA"},"outputs":[],"source":["questions = pd.read_csv(FN_QUESTIONS)\n","contexts  = pd.read_csv(FN_CONTEXTS)"]},{"cell_type":"markdown","metadata":{"id":"zUUz0lb2yLAg"},"source":["### Binary View Dataframes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuLRs3tXkb_C"},"outputs":[],"source":["content_a2_raw = pd.read_csv(FN_A2_RAW)\n","content_a2_fa  = pd.read_csv(FN_A2_FA)\n","content_a3_fa  = pd.read_csv(FN_A3_FA)\n","content_a3_maj = pd.read_csv(FN_A3_MAJ)\n","\n","# Could also try this with using the 3's as 0's\n","content_a2_raw = content_a2_raw[content_a2_raw['answer'] != 3] \n","content_a2_fa  = content_a2_fa[content_a2_fa['answer'] != 3] \n","content_a3_fa  = content_a3_fa[content_a3_fa['answer'] != 3] \n","content_a3_maj = content_a3_maj[content_a3_maj['answer'] != 3] \n","\n","content_a3_majplus = pd.concat([content_a3_fa, content_a3_maj])\n","content_a3a2_mp    = pd.concat([content_a3_fa, content_a3_maj, content_a2_fa])"]},{"cell_type":"code","source":["# We need the per-question and per-context dataframes, as well. \n","question_ids = content_a3a2_mp['question_id'].unique()\n","context_ids = content_a3a2_mp['context_id'].unique()\n","# These lists will allow us to map back to the original statements\n","# and need to be saved. "],"metadata":{"id":"jisQZ47_rlzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question_dfs = []\n","for g_id, group in content_a3a2_mp.groupby(by=['question_id']):\n","  print(g_id, len(group))\n","\n","# So we can just do the above when we make the dataloaders. "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8AAfK7fsht8","executionInfo":{"status":"ok","timestamp":1680284197752,"user_tz":240,"elapsed":7,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}},"outputId":"0a7d1e3e-6ff3-479c-9361-0fce64d5a69b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5 839\n","6 589\n","7 591\n","8 633\n","10 641\n","11 556\n","12 492\n"]}]},{"cell_type":"markdown","metadata":{"id":"7CQLt7Gb717p"},"source":["### Multi-label View Dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cf7EXtpPZxC4"},"outputs":[],"source":["question_ids = content_a3a2_mp['question_id'].unique()\n","\n","# Data for Multi-Label Classification Models.\n","df_a2fa_mc   = content_a2_fa.groupby(by=['context_id', 'comment_id'])\n","df_a3mp_mc   = content_a3_majplus.groupby(by=['context_id', 'comment_id'])\n","df_a3a2mp_mc = content_a3a2_mp.groupby(by=['context_id', 'comment_id'])"]},{"cell_type":"markdown","metadata":{"id":"kNQBRgQkkb_B"},"source":["## LLM Initalization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357,"referenced_widgets":["43bc96a1f53647d8b1a2c54cb708ab02","df14f7415fbe4db59fee86541a0ee42e","506417c0d95e420e93769e821dadc5da","b422d514d994460fbbf5b594ab8ee9bb","78ea5f30e06c4f7fa6427f61ee363dab","2548eeedc295474aa327177691adb79d","ec977e2b733e44739d761b73fd1dc40e","fe7df1ac2fd842e7a960991dd2453fbc","7da463b24577492c94ec4db66daa7f13","00a9d51db6d24f5d896f2bb72cf7981e","febd3c4ce4944bb8801a2262d4876591","0c8dbc17bcdb4aec8d6072bb7b58edd7","032e7ccfac8b43daa781050c258052ad","868a6b1ff59a450e9069449e04ca04c5","b535cd62d1514c6c9aa4466fb20e69ba","5e1c661a0bbb434c9ef74aaca877605b","181f583949c2460a915c4bfed55402f4","913d049a6ad24c19bf7becb01a9ec614","2b510c19f01c4b7ab36ff3350551b826","1af7a797c122452da0d6fa697a7abb05","d32c440a4b46460d919133ce2ae9b325","0caf4f0edc784bfcb2c85ffa831da4fc","404a91190070408faa663574782ca700","3cb934de44774180b405cfd92f4cbceb","bcb05d0e8e7f49ea981688851811a19a","6181001185634d7c81f1fbd355187505","a7b2d8d3160d455ea44c27224918197e","6bb8aa0d808042f5a5faaf91328c2d61","281c293c85394623835f62da3fa4235d","e68665a1844b4d23899bc4c03ea65e5a","13b029a1b54844ae9d5eca208a37f70d","d209895c968847bebdb4c7ab9fb6392d","a74a2f8810b345da83ad69ec25502a92"]},"executionInfo":{"elapsed":7192,"status":"ok","timestamp":1680284205241,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"wrotnPAHkb_C","outputId":"2d299861-4c7f-4bd7-ff59-8d5e79d735b5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43bc96a1f53647d8b1a2c54cb708ab02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c8dbc17bcdb4aec8d6072bb7b58edd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"404a91190070408faa663574782ca700"}},"metadata":{}}],"source":["# For DistilBERT:\n","model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n","\n","## Want BERT instead of distilBERT? Uncomment the following line:\n","#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","\n","# TODO - figure out the above for the Q/A model. Might need a different\n","#        enough implementation/approach that it warrents a dif script. \n","#        It takes a masked input for the Q and options for multiple choice? idk. \n","\n","# Load pretrained model/tokenizer\n","tokenizer  = tokenizer_class.from_pretrained(pretrained_weights, truncate=True,)"]},{"cell_type":"markdown","metadata":{"id":"noOpZRP8yXpr"},"source":["## Question/Context Embedding Dicts"]},{"cell_type":"markdown","metadata":{"id":"IOoeD1gRUkyE"},"source":["Used later to build the individual training examples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad4YwoEX2otO"},"outputs":[],"source":["def build_ts_am_map(df_in, text_col_name, id_col):\n","  ids   = df_in[id_col].values\n","  sents = df_in[text_col_name].values\n","\n","  token_seqs = []\n","  attn_masks = []\n","  id_2_tsam = dict()\n","  for id, sent in zip(ids, sents):\n","    enc_dict = tokenizer.encode_plus(\n","      sent,\n","      add_special_tokens = False,\n","      max_length = MAX_SEQ_LEN, \n","      pad_to_max_length = True,\n","      return_attention_mask = True,\n","      return_tensors = 'pt'\n","    )\n","    id_2_tsam[id] = [enc_dict['input_ids'], enc_dict['attention_mask']]\n","\n","  return id_2_tsam"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680284205242,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"35ukWIxGuq_t","outputId":"7234f39a-9a44-42bc-8285-641fbabfc090"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]}],"source":["map_q_2_ts_am   = build_ts_am_map(questions, 'question_text', 'question_id')\n","map_ctx_2_ts_am = build_ts_am_map(contexts, 'context_text', 'context_id')"]},{"cell_type":"markdown","metadata":{"id":"Ee2A1EDh0Zgd"},"source":["## Balancing - Over-Sampling Method\n","\n","Tweaked for MC."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8b-jBO3zaM9"},"outputs":[],"source":["# The ctx_idx will change depending on if this is being passed Binary or MC data\n","# 2 => MC\n","# ? => Binary\n","def max_size_oversampling_wr(examples, ctx_idx=2):\n","  question_counts = dict()\n","  context_counts  = dict()\n","  \n","  for ex in examples:\n","    ctx = ex[ctx_idx]\n","    if ctx not in context_counts:\n","      context_counts[ctx] = 0\n","    context_counts[ctx] += 1\n","\n","  context_examples    = { k: [] for k in context_counts}\n","  context_populations = { k: [] for k in context_counts}\n","\n","  for example in examples:\n","    ctx = example[ctx_idx] # This might change depending on shape of examples.\n","    context_populations[ctx].append(example)\n","\n","  max_size = 0\n","  for k in context_populations:\n","    size_pop = len(context_populations[k])\n","    if size_pop > max_size:\n","      max_size = size_pop\n","\n","  for k in context_examples:\n","    while len(context_examples[k]) < max_size:\n","      context_examples[k].append(random.sample(context_populations[k], 1)[0])\n","\n","  example_tuples_max_sampled = []\n","  for k in context_examples:\n","    for ex in context_examples[k]:\n","      example_tuples_max_sampled.append(ex)\n","  return example_tuples_max_sampled"]},{"cell_type":"markdown","metadata":{"id":"7EFQMMop0fyz"},"source":["## DataLoaders\n"]},{"cell_type":"markdown","metadata":{"id":"EiLNu1dR9rLf"},"source":["### Multi-Label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"at-9T_v0h60u"},"outputs":[],"source":["def create_example_tuples_mc(df_content, question_ids, balance=False):\n","  example_tuples = []\n","  \n","  for (context_id, comment_id), group in df_content:\n","    content = group['content_text'].values[0]\n","\n","    g_anss = []\n","    for q_id in question_ids:\n","      q_ans = group[group['question_id'] == q_id]['answer'].values\n","      if len(q_ans) > 0 and q_ans[0] != 3:\n","        g_anss.append(q_ans[0])\n","      else:\n","        g_anss.append(0)\n","\n","    example_tuples.append([\n","      map_ctx_2_ts_am[context_id],\n","      content,\n","      torch.reshape(torch.tensor(g_anss), (1, len(g_anss))),\n","      context_id\n","    ])\n","\n","  if balance:\n","    example_tuples = max_size_oversampling_wr(example_tuples, ctx_idx=2)\n","\n","  labels = [i[2] for i in example_tuples]\n","  iids_contexts  = []\n","  iids_contents  = []\n","  ams_contexts  = []\n","  ams_contents  = []\n","\n","  for [ts_ctx, am_ctx], sent, _, _ in example_tuples:\n","    iids_contexts.append(ts_ctx)\n","    ams_contexts.append(am_ctx)\n","\n","    enc_dict = tokenizer.encode_plus(\n","        sent,\n","        add_special_tokens = False,\n","        max_length = MAX_SEQ_LEN, \n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","    iids_contents.append(enc_dict['input_ids'])\n","    ams_contents.append(enc_dict['attention_mask'])\n","  \n","  iids_contexts  = torch.cat(iids_contexts,  dim=0)\n","  iids_contents  = torch.cat(iids_contents,  dim=0)\n","  ams_contexts  =  torch.cat(ams_contexts,  dim=0)\n","  ams_contents  =  torch.cat(ams_contents,  dim=0)\n","  labels = torch.cat(labels, dim=0)\n","  labels = labels.to(torch.float64)\n","  ret_obj = [\n","      iids_contexts, \n","      ams_contexts, \n","      iids_contents, \n","      ams_contents,\n","      labels\n","  ]\n","  return ret_obj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wY3j6nyR0py9"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","\n","# Note - I think that does it? Gross looking, but straightforward?\n","def create_dataloaders_mc(df_content, question_ids, test_ratio, balance=True):\n","  bigbagohtensors = create_example_tuples_mc(df_content, question_ids, balance=balance)\n","  \n","  iids_ctx = bigbagohtensors[0]\n","  ams_ctx  = bigbagohtensors[1]\n","  iids_cnt = bigbagohtensors[2] \n","  ams_cnt  = bigbagohtensors[3] \n","  labels   = bigbagohtensors[4]  \n","\n","  if test_ratio != 0:\n","    train_idx, test_idx = train_test_split(\n","        np.arange(len(labels)),\n","        test_size = test_ratio,\n","        shuffle = True)\n","    \n","    # \"Hey bill, should you google about the splat operator? Maybe.\"\n","    train_set = TensorDataset(iids_ctx[train_idx],\n","                              ams_ctx[train_idx],\n","                              iids_cnt[train_idx],\n","                              ams_cnt[train_idx],\n","                              labels[train_idx])\n","\n","    test_set = TensorDataset(iids_ctx[test_idx],\n","                            ams_ctx[test_idx],\n","                            iids_cnt[test_idx],\n","                            ams_cnt[test_idx],\n","                            labels[test_idx])\n","\n","    train_dataloader = DataLoader(train_set,\n","                                  batch_size=BATCH_SIZE)\n","\n","    test_dataloader = DataLoader(test_set,\n","                                batch_size=BATCH_SIZE)\n","\n","    return train_dataloader, test_dataloader\n","  else:\n","    train_set = TensorDataset(iids_ctx, ams_ctx, iids_cnt, ams_cnt, labels)\n","    train_dataloader = DataLoader(train_set,\n","                                  batch_size=BATCH_SIZE)\n","\n","    return train_dataloader"]},{"cell_type":"markdown","metadata":{"id":"5SBoofED-TbS"},"source":["### Binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVQsgdTe-uWA"},"outputs":[],"source":["def create_example_tuples_bin(df_content, balance=False):\n","  example_tuples = []\n","  \n","  # Uses the maps created before to obtain token_seqs and attn_masks \n","  # for the questions and contexts. \n","  def make_make_tuple(out_list):\n","    def make_tuple(row):\n","      out_list.append([\n","          map_q_2_ts_am[row['question_id']],\n","          map_ctx_2_ts_am[row['context_id']],\n","          row['content_text'],\n","          row['answer'],\n","          row['context_id'],\n","          row['question_id']\n","      ])\n","    return make_tuple\n","  \n","  df_content.apply(make_make_tuple(example_tuples), axis=1)\n","\n","  if balance:\n","    example_tuples = max_size_oversampling_wr(example_tuples, ctx_idx=4)\n","\n","  labels = [i[3] for i in example_tuples]\n","  iids_questions = []\n","  iids_contexts  = []\n","  iids_contents  = []\n","  ams_questions = []\n","  ams_contexts  = []\n","  ams_contents  = []\n","\n","  for [ts_q, am_q], [ts_ctx, am_ctx], sent, _, _, _ in example_tuples:\n","    iids_questions.append(ts_q)\n","    iids_contexts.append(ts_ctx)\n","    ams_questions.append(am_q)\n","    ams_contexts.append(am_ctx)\n","\n","    enc_dict = tokenizer.encode_plus(\n","        sent,\n","        add_special_tokens = False,\n","        max_length = MAX_SEQ_LEN, \n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","    iids_contents.append(enc_dict['input_ids'])\n","    ams_contents.append(enc_dict['attention_mask'])\n","  \n","  iids_questions = torch.cat(iids_questions, dim=0)\n","  iids_contexts  = torch.cat(iids_contexts,  dim=0)\n","  iids_contents  = torch.cat(iids_contents,  dim=0)\n","  ams_questions =  torch.cat(ams_questions, dim=0)\n","  ams_contexts  =  torch.cat(ams_contexts,  dim=0)\n","  ams_contents  =  torch.cat(ams_contents,  dim=0)\n","  labels = torch.tensor(labels)\n","  labels = torch.reshape(labels, (-1,1))\n","  labels = labels.to(torch.float64)\n","  ret_obj = [\n","      iids_questions, \n","      ams_questions, \n","      iids_contexts, \n","      ams_contexts, \n","      iids_contents, \n","      ams_contents,\n","      labels\n","  ]\n","  return ret_obj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBs0G70x-uWM"},"outputs":[],"source":["def create_dataloaders_bin(df_content, test_ratio, balance=False):\n","  bigbagohtensors = create_example_tuples_bin(df_content, balance=balance)\n","  \n","  iids_q   = bigbagohtensors[0]\n","  ams_q    = bigbagohtensors[1] \n","  iids_ctx = bigbagohtensors[2]\n","  ams_ctx  = bigbagohtensors[3]\n","  iids_cnt = bigbagohtensors[4] \n","  ams_cnt  = bigbagohtensors[5] \n","  labels   = bigbagohtensors[6]  \n","\n","  if test_ratio != 0:\n","    train_idx, test_idx = train_test_split(\n","        np.arange(len(labels)),\n","        test_size = test_ratio,\n","        shuffle = True,\n","        stratify = labels)\n","    \n","    # \"Hey bill, should you google about the splat operator? Maybe.\"\n","    train_set = TensorDataset(iids_q[train_idx],\n","                              ams_q[train_idx],\n","                              iids_ctx[train_idx],\n","                              ams_ctx[train_idx],\n","                              iids_cnt[train_idx],\n","                              ams_cnt[train_idx],\n","                              labels[train_idx])\n","\n","    test_set = TensorDataset(iids_q[test_idx],\n","                            ams_q[test_idx],\n","                            iids_ctx[test_idx],\n","                            ams_ctx[test_idx],\n","                            iids_cnt[test_idx],\n","                            ams_cnt[test_idx],\n","                            labels[test_idx])\n","\n","    train_dataloader = DataLoader(train_set,\n","                                  batch_size=BATCH_SIZE)\n","\n","    test_dataloader = DataLoader(test_set,\n","                                batch_size=BATCH_SIZE)\n","    \n","    return train_dataloader, test_dataloader\n","  else:\n","    train_set = TensorDataset(iids_q, ams_q, iids_ctx, ams_ctx, iids_cnt, ams_cnt, labels)\n","    train_dataloader = DataLoader(train_set,\n","                                  batch_size=BATCH_SIZE)\n","    return train_dataloader"]},{"cell_type":"markdown","metadata":{"id":"rPCF7MZNzOAT"},"source":["# Model Implementations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYqqeGe8zVd8"},"outputs":[],"source":["import torch.nn.functional as F\n","from torch.nn import Module, RNN, Linear, BCELoss, LogSoftmax, BatchNorm1d\n","\n","NUM_BERT_FEATURES = 768\n","NUM_QUESTIONS = 7\n","\n","HL_QUESTION = 128\n","HL_CONTEXT = 128\n","HL_CONTENT = 128\n","HL_PRED = 8"]},{"cell_type":"markdown","metadata":{"id":"BEHErFubuNSt"},"source":["## AQAM-MC - Content-Only\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ychYUGstuNFp"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQAM_MC_C(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers\n","        self.pred_linear1 = Linear(HL_CONTENT, HL_PRED)\n","        \n","        # Just needed to change from 1 to NUM_QUESTIONS?\n","        self.pred_linear2 = Linear(HL_PRED, NUM_QUESTIONS)\n","\n","        # Batch Norms\n","        self.bn_cnt  = BatchNorm1d(HL_CONTENT) \n","\n","    def forward(self, data):\n","        tok_seq_cnt, attn_mask_cnt = data[2], data[3]\n","        \n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        # ls_cnt = self.bn_cnt(ls_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","  \n","        # Prediction Layers\n","        ls_cnt = self.pred_linear1(ls_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","        return self.pred_linear2(ls_cnt)"]},{"cell_type":"markdown","metadata":{"id":"QpIXGQzpuh3L"},"source":["## AQAM-MC - Content-Context\n"]},{"cell_type":"code","execution_count":187,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1680310077924,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"fBEE3EGSuh3R"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQAM_MC_CC(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        self.context_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTEXT)\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers\n","        self.pred_linear1 = Linear(HL_CONTEXT+HL_CONTENT, HL_PRED)\n","        self.pred_linear2 = Linear(HL_PRED, NUM_QUESTIONS)\n","\n","        # Batch Norms\n","        self.bn_cnt  = BatchNorm1d(HL_CONTENT) \n","        self.bn_ctx  = BatchNorm1d(HL_CONTEXT)\n","        self.bn_cat  = BatchNorm1d(HL_PRED) \n","\n","    def forward(self, data):\n","        tok_seq_ctx, attn_mask_ctx = data[0], data[1]\n","        tok_seq_cnt, attn_mask_cnt = data[2], data[3]\n","        \n","        emb_ctx = self.bert_layer(tok_seq_ctx, \n","                                  attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","\n","        ls_ctx = self.context_linear1(emb_ctx)\n","        # ls_ctx = self.bn_ctx(ls_ctx)\n","        ls_ctx = F.relu(ls_ctx)\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        # ls_cnt = self.bn_cnt(ls_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","\n","        # We'd change aggregation here.\n","        ls_cat = torch.cat((ls_ctx, ls_cnt), dim=1)\n","        \n","        # Prediction Layers\n","        ls_cat = self.pred_linear1(ls_cat)\n","        ls_cat = F.relu(ls_cat)\n","        return self.pred_linear2(ls_cat)  # Needs to output a BATCH x 7 tensor"]},{"cell_type":"markdown","metadata":{"id":"x6sTgOXpAQRP"},"source":["## AQAM-Binary - Content-Question\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIvGMex1AQRV"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQAM_B_CQ(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        self.q_linear1       = Linear(NUM_BERT_FEATURES, HL_QUESTION)\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers\n","        self.pred_linear1 = Linear(HL_QUESTION+HL_CONTENT, HL_PRED)\n","        self.pred_linear2 = Linear(HL_PRED, 1)\n","\n","        # Batch Norms\n","        self.bn_q    = BatchNorm1d(HL_QUESTION)\n","        self.bn_cnt  = BatchNorm1d(HL_CONTENT) \n","\n","        self.bn_cat  = BatchNorm1d(HL_PRED) \n","\n","    def forward(self, data):\n","        tok_seq_q,   attn_mask_q   = data[0], data[1]\n","        # tok_seq_ctx, attn_mask_ctx = data[2], data[3]\n","        tok_seq_cnt, attn_mask_cnt = data[4], data[5]\n","        \n","        emb_q   = self.bert_layer(tok_seq_q,   \n","                                  attention_mask=attn_mask_q)[0][:, 0, :]\n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","\n","        ls_q   = self.q_linear1(emb_q)\n","        # ls_q   = self.bn_q(ls_q)\n","        ls_q   = F.relu(ls_q)\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        # ls_cnt = self.bn_cnt(ls_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","\n","        ls_cat = torch.cat((ls_q, ls_cnt), dim=1)\n","        \n","        # Prediction Layers\n","        ls_cat = self.pred_linear1(ls_cat)\n","        # ls_cat = self.bn_cat(ls_cat)\n","        ls_cat = F.relu(ls_cat)\n","        return self.pred_linear2(ls_cat)"]},{"cell_type":"markdown","metadata":{"id":"t4re1yTiAdgZ"},"source":[]},{"cell_type":"markdown","metadata":{"id":"TbrkrVp9unHC"},"source":["## AQAM-Binary - Content-Context-Question\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDvRHzV2unHF"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQAM_B_CCQ(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        # 3 MLP Stacks\n","        self.q_linear1   = Linear(NUM_BERT_FEATURES, HL_QUESTION)\n","        self.ctx_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTEXT)\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers\n","        self.pred_linear1 = Linear(HL_QUESTION+HL_CONTEXT+HL_CONTENT, HL_PRED)\n","        self.pred_linear2 = Linear(HL_PRED, 1)\n","\n","        # Batch Norms\n","        self.bn_q    = BatchNorm1d(HL_QUESTION)\n","        self.bn_ctx  = BatchNorm1d(HL_CONTEXT)\n","        self.bn_cnt  = BatchNorm1d(HL_CONTENT) \n","\n","        # self.bn_triple = BatchNorm1d(HL_QUESTION+HL_CONTEXTS+HL_CONTENT)\n","        self.bn_cat  = BatchNorm1d(HL_PRED) \n","\n","    def forward(self, data):\n","        tok_seq_q,   attn_mask_q   = data[0], data[1]\n","        tok_seq_ctx, attn_mask_ctx = data[2], data[3]\n","        tok_seq_cnt, attn_mask_cnt = data[4], data[5]\n","        \n","        emb_q   = self.bert_layer(tok_seq_q,   \n","                                  attention_mask=attn_mask_q)[0][:, 0, :]\n","        emb_ctx = self.bert_layer(tok_seq_ctx, \n","                                  attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","\n","        # The 3 Heads - Question, Context, Content spaces.\n","        ls_q   = self.q_linear1(emb_q)\n","        # ls_q   = self.bn_q(ls_q)\n","        ls_q   = F.relu(ls_q)\n","\n","        ls_ctx = self.ctx_linear1(emb_ctx)\n","        # ls_ctx = self.bn_ctx(ls_ctx)\n","        ls_ctx = F.relu(ls_ctx)\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        # ls_cnt = self.bn_cnt(ls_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","\n","        ls_cat = torch.cat((ls_q, ls_ctx, ls_cnt), dim=1)\n","        \n","        # Prediction Layers\n","        ls_cat = self.pred_linear1(ls_cat)\n","        # ls_cat = self.bn_cat(ls_cat)\n","        ls_cat = F.relu(ls_cat)\n","        return self.pred_linear2(ls_cat)"]},{"cell_type":"markdown","metadata":{"id":"h6ilu9ax8bze"},"source":["# Training and Evaluation Method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynT43r7n9s2P"},"outputs":[],"source":["def train_and_evaluate(model, metric, training_batches, test_set_batches, epochs, cuda_dev):\n","  optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n","  criterion = torch.nn.BCEWithLogitsLoss().to(cuda_dev)\n","\n","  train_accs = []\n","  test_accs  = [[] for _ in test_set_batches]\n","  for epoch in tqdm(range(epochs), unit=\"epoch\"):\n","    model.train()\n","    \n","    batch_accs = []\n","    for batch in training_batches:\n","      batch = tuple(t.to(cuda_dev) for t in batch)\n","      labels = batch[-1]\n","\n","      optimizer.zero_grad()\n","      out = model(batch)\n","      loss = criterion(out, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      batch_accs.append(metric(out, labels))\n","\n","    train_accs.append(sum(batch_accs)/float(len(batch_accs)))\n","\n","    model.eval()\n","    for t_idx, test_batches in enumerate(test_set_batches):\n","      test_batch_accs = []\n","      for test_batch in test_batches:\n","        test_batch = tuple(t.to(cuda_dev) for t in test_batch)\n","        test_labels = test_batch[-1]\n","\n","        test_out = model(test_batch)\n","        test_batch_accs.append(metric(test_out, test_labels))\n","      \n","      test_accs[t_idx].append(sum(test_batch_accs)/float(len(test_batch_accs)))\n","\n","  return train_accs, test_accs, model"]},{"cell_type":"markdown","metadata":{"id":"4F9mZnh3xKZF"},"source":["# Embedding Extraction Methods"]},{"cell_type":"markdown","metadata":{"id":"8Yj20aAZxOPt"},"source":["These take in a model and a dataset, and return the underlying BERT/LLM embeddings for each part of the input tuple for each item/batch in the dataset. Note - This assumes a structure to the model, or atleast that it contains a LLM layer named \"bert_layer\". More complicated architectures (like ones that use a different LLM for context/content/question) will need a different 'gather' method to pull out the embeddings. "]},{"cell_type":"markdown","metadata":{"id":"HQROFGFECBfT"},"source":["## Multi-Label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zl9uPMYt4CKw"},"outputs":[],"source":["def gather_embeddings_MC(model, dataloaders):\n","  model.eval()\n","\n","  embs_cnt     = []\n","  embs_cntctx  = []\n","  agree_scores  = []\n","  for dl in dataloaders:\n","    for batch in dl:\n","      tok_seq_ctx, attn_mask_ctx = batch[0].to(cuda_dev), batch[1].to(cuda_dev)\n","      tok_seq_cnt, attn_mask_cnt = batch[2].to(cuda_dev), batch[3].to(cuda_dev)\n","\n","      labels = batch[4].to(cuda_dev)\n","      batch = [b.to(cuda_dev) for b in batch]\n","\n","      with torch.no_grad():\n","        emb_cnt = model.bert_layer(tok_seq_cnt, \n","                                      attention_mask=attn_mask_cnt)[0][:, 0, :]\n","        emb_ctx = model.bert_layer(tok_seq_ctx, \n","                                      attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        out = model(batch)\n","\n","        y_pred_tag = torch.round(torch.sigmoid(out))\n","        # y_pred_tag should be BATCH_SIZEx7, now.\n","        # print(y_pred_tag.shape)\n","\n","        agree = y_pred_tag == labels\n","        # agree should also be BATCH_SIZEx7 now. \n","        # print(agree.shape)\n","\n","        # Need to sum over the second dimension.\n","        agree = torch.sum(agree, dim=1)\n","        # print(agree.shape)\n","\n","      embs_cnt.append(emb_cnt.cpu())\n","      embs_cntctx.append(torch.cat([emb_cnt.cpu(), emb_ctx.cpu()], 1))\n","      agree_scores.append(agree.cpu())\n","      # Overkill, but hey, not going OoM on the gpu anymore.\n","      del tok_seq_cnt\n","      del tok_seq_ctx\n","      del attn_mask_cnt\n","      del attn_mask_ctx\n","      del emb_cnt\n","      del emb_ctx\n","      del batch\n","      with torch.no_grad(): # Note to future bill: empty cache only works in a no_grad\n","        torch.cuda.empty_cache()\n","  \n","  return torch.cat(embs_cnt, 0), torch.cat(embs_cntctx, 0), torch.cat(agree_scores, 0)"]},{"cell_type":"markdown","metadata":{"id":"2kxrcLbaCRmg"},"source":["## Binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guoAacFHCJCq"},"outputs":[],"source":["def gather_embeddings_bin(model, dataloaders):\n","  model.eval()\n","\n","  embs_cnt     = []\n","  embs_cntctx  = []\n","  embs_cntq    = []\n","  embs_cntctxq = []\n","  agree_flags  = []\n","  for dl in dataloaders:\n","    for batch in dl:\n","      tok_seq_q,   attn_mask_q   = batch[0].to(cuda_dev), batch[1].to(cuda_dev)\n","      tok_seq_ctx, attn_mask_ctx = batch[2].to(cuda_dev), batch[3].to(cuda_dev)\n","      tok_seq_cnt, attn_mask_cnt = batch[4].to(cuda_dev), batch[5].to(cuda_dev)\n","\n","      labels = batch[6].to(cuda_dev)\n","      batch = [b.to(cuda_dev) for b in batch]\n","      with torch.no_grad():\n","        emb_cnt = model.bert_layer(tok_seq_cnt, \n","                                      attention_mask=attn_mask_cnt)[0][:, 0, :]\n","        emb_ctx = model.bert_layer(tok_seq_ctx, \n","                                      attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_q   = model.bert_layer(tok_seq_q,\n","                                      attention_mask=attn_mask_q)[0][:, 0, :]\n","        out = model(batch)\n","        y_pred_tag = torch.round(torch.sigmoid(out))\n","        agree = y_pred_tag == labels\n","\n","      # TODO - Here is where we'll call the full forward pass of the model on\n","      #        the batch, and then generate an agreement vector? flag vector\n","      #        where a_i is 0 if the model disagrees, 1 if it agrees.\n","      # so we get an output, and we want to get the 'labels == output' flags.\n","\n","      embs_cnt.append(emb_cnt.cpu())\n","      embs_cntctx.append(torch.cat([emb_cnt.cpu(), emb_ctx.cpu()], 1))\n","      embs_cntq.append(torch.cat([emb_cnt.cpu(), emb_q.cpu()], 1))\n","      embs_cntctxq.append(torch.cat([emb_cnt.cpu(), emb_ctx.cpu(), emb_q.cpu()], 1))\n","      agree_flags.append(agree.cpu())\n","      # Overkill, but hey, not going OoM on the gpu anymore.\n","      del tok_seq_q\n","      del tok_seq_cnt\n","      del tok_seq_ctx\n","      del attn_mask_q\n","      del attn_mask_cnt\n","      del attn_mask_ctx\n","      del emb_cnt\n","      del emb_ctx\n","      del emb_q\n","      del batch\n","      with torch.no_grad(): # Note to future bill: empty cache only works in a no_grad\n","        torch.cuda.empty_cache()\n","  \n","  return torch.cat(embs_cnt, 0), torch.cat(embs_cntctx, 0), torch.cat(embs_cntq, 0), torch.cat(embs_cntctxq, 0), torch.cat(agree_flags, 0)"]},{"cell_type":"markdown","metadata":{"id":"1UhRSuE9wQHp"},"source":["# Experiments"]},{"cell_type":"markdown","metadata":{"id":"wbxhAQwswYDH"},"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkM6UKd4xkr7"},"outputs":[],"source":["cuda_dev  = torch.device(\"cuda:0\")"]},{"cell_type":"markdown","metadata":{"id":"ZhqZxzTVHUaH"},"source":["### Multi-Label Models"]},{"cell_type":"code","execution_count":176,"metadata":{"executionInfo":{"elapsed":8098,"status":"ok","timestamp":1680309390353,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"NWCski_kwUCl"},"outputs":[],"source":["A23_fa_train, A23_fa_test = create_dataloaders_mc(df_a3a2mp_mc, \n","                                                  question_ids,\n","                                                  0.2,\n","                                                  balance=True)\n","\n","A3_mp_train, A3_mp_test = create_dataloaders_mc(df_a3mp_mc, \n","                                                question_ids,\n","                                                0.2,\n","                                                balance=False)\n","A2_fa_train, A2_fa_test = create_dataloaders_mc(df_a2fa_mc,  \n","                                                question_ids,\n","                                                0.2, \n","                                                balance=False)\n","\n","# # For generating the 'full set' of tSNE embeddings. \n","A3_a, A3_b = create_dataloaders_mc(df_a3mp_mc, question_ids, \n","                                0.5,\n","                                balance=False)\n","A2_a, A2_b = create_dataloaders_mc(df_a2fa_mc, question_ids, \n","                                0.5,\n","                                balance=False)"]},{"cell_type":"code","execution_count":177,"metadata":{"executionInfo":{"elapsed":521,"status":"ok","timestamp":1680309394700,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"scdpIS-8QcOx"},"outputs":[],"source":["# To avoid making a new one on each call. \n","ml_metric = MultilabelAccuracy(num_labels=NUM_QUESTIONS).to(cuda_dev)\n","\n","def mlc_acc(y_pred, y_test):\n","  return ml_metric(y_pred, y_test).item()"]},{"cell_type":"code","source":["# We don't need custom dataloaders for these, we just need to look at the\n","# accuracy for each element in the output tensor. Maybe this ml_metric\n","# has a way to output that? \n","# yea, looking at the docs: https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#multilabelaccuracy\n","# we need to set the average to None and we'll get an output of shape (C,)\n","# where C is the number of classes, in this case, questions.\n","\n","ml_metric_pc = MultilabelAccuracy(num_labels=NUM_QUESTIONS,\n","                                  average=None).to(cuda_dev)\n","\n","dl_perqacc = create_dataloaders_mc(df_a3a2mp_mc, question_ids, 0.0)\n","\n","# We will still need to do something about the per-context? \n","dls_ml_contexts = []\n","for g_id, g_df in content_a3a2_mp.groupby(by=['context_id']):\n","  g_dfgb = g_df.groupby(by=['context_id', 'comment_id'])\n","  g_dl = create_dataloaders_mc(g_dfgb, question_ids, 0)\n","  dls_ml_contexts.append([g_id, g_dl])"],"metadata":{"id":"ig6hVy2_wruA","executionInfo":{"status":"ok","timestamp":1680309408993,"user_tz":240,"elapsed":5636,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"execution_count":178,"outputs":[]},{"cell_type":"code","source":["def per_q_ml_acc(ml_model, dataloader):\n","  ml_model.eval()\n","  batch_accs = []\n","  for batch in dataloader:\n","    batch = tuple(t.to(cuda_dev) for t in batch)\n","    labels = batch[-1]\n","    batch_accs.append(ml_metric_pc(ml_model(batch), labels))\n","  return torch.mean(torch.stack(batch_accs, dim=1), dim=1)\n","\n","def per_c_ml_acc(ml_model, ctx_dataloaders):\n","  ml_model.eval()\n","  ctx_accs = []\n","  for ctx_id, dl in ctx_dataloaders:\n","    batch_accs = []\n","    for batch in dl:\n","      batch = tuple(t.to(cuda_dev) for t in batch)\n","      labels = batch[-1]\n","      batch_accs.append(mlc_acc(ml_model(batch), labels))\n","    ctx_accs.append([ctx_id, sum(batch_accs)/float(len(batch_accs))])\n","  return ctx_accs"],"metadata":{"id":"jey80zqi3tts","executionInfo":{"status":"ok","timestamp":1680311045790,"user_tz":240,"elapsed":321,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"execution_count":201,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YBQfX1cKwsyQ"},"source":["#### C-Model"]},{"cell_type":"code","execution_count":206,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["82d78dde5efa455a950ca1913d7c231e","eb77d9784ca74abdb5cb75f6e8e55f71","2a48aa7d3c714ab7b85c94ef5d8625c5","f640a59bacd94604a9b1516fa62b51ff","60fedf701b9943b3ac7eec7d892c3cb6","efdcacae65cf43d1b62e10d0a5ba79a3","9e2241817da24b009e15cc6b3c9098c4","78451b1aca5d479797e9df5bfe4d6dcc","5e78745eb9294cba891a2da3c70132df","38654dc5ab9c49d8854e1e6affa31fba","190ceaa536d54e03a90c0a3b935d121e"]},"executionInfo":{"elapsed":218870,"status":"ok","timestamp":1680311372247,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"sknYT1CKx307","outputId":"a17a9102-0844-4d7f-d0bb-979764cbab98"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/23 [00:00<?, ?epoch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d78dde5efa455a950ca1913d7c231e"}},"metadata":{}}],"source":["a23_mc_c_model = AQAM_MC_C().to(cuda_dev)\n","\n","embeds_a23c_pre = gather_embeddings_MC(a23_mc_c_model, [A3_a, A3_b, A2_a, A2_b])\n","train_a23c, test_a23c, a23_mc_c_model = train_and_evaluate(\n","    a23_mc_c_model,\n","    mlc_acc,\n","    A23_fa_train, \n","    [A23_fa_test, A3_mp_test, A2_fa_test],\n","    EPOCHS-2,\n","    cuda_dev)\n","embeds_a23c_post = gather_embeddings_MC(a23_mc_c_model, [A3_a, A3_b, A2_a, A2_b])"]},{"cell_type":"code","source":["a23_mc_c_per_q_accs   = per_q_ml_acc(a23_mc_c_model, dl_perqacc)\n","a23_mc_c_per_ctx_accs = per_c_ml_acc(a23_mc_c_model, dls_ml_contexts)"],"metadata":{"id":"LkD3qrEu6Fl0","executionInfo":{"status":"ok","timestamp":1680311379022,"user_tz":240,"elapsed":6785,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"execution_count":207,"outputs":[]},{"cell_type":"code","source":["print(\"Content-Only\")\n","for q, acc in zip(question_ids, a23_mc_c_per_q_accs.tolist()):\n","  print(f\"  q {q:2d},  acc: {acc:0.8f}\")\n","print(\"---\")\n","for ctx, acc in a23_mc_c_per_ctx_accs:\n","  print(f\"ctx {ctx}, acc: {acc:0.8f}\")"],"metadata":{"id":"MOLFUt-A76vO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZDawadwHEPW"},"source":["#### CC-Model"]},{"cell_type":"code","execution_count":188,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["1192de6859bd4daebbbb6c6704713989","abbf21510fd64d90b9dad93a6feb5e8e","3f83fca3703b4a5ab1bff68fe3ddca93","38197922f4ae4e559d4a5fb9a20399c4","d2f37c3b7fa047ad9f6a041b3c36366d","6ab70251a855413f837835d27dd7da05","1cb1bf9ac9f84680a99241d89a0fba1c","eea286a034aa461eb813768d506aff76","e07bc32a01d94fadad09935a78f87516","ec1f3ccdf9a940288e3dfc1006fb0eb9","28ae974309f840409a6e40a9dc5c3d8e"]},"executionInfo":{"elapsed":459077,"status":"ok","timestamp":1680310545549,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"zDT6UloYHEPe","outputId":"542f9ccd-c43a-435a-a55e-b832edec7ba5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1192de6859bd4daebbbb6c6704713989"}},"metadata":{}}],"source":["a23_mc_cc_model = AQAM_MC_CC().to(cuda_dev)\n","\n","embeds_a23cc_pre = gather_embeddings_MC(a23_mc_cc_model, [A3_a, A3_b, A2_a, A2_b])\n","train_a23cc, test_a23cc, a23_mc_cc_model = train_and_evaluate(\n","    a23_mc_cc_model,\n","    mlc_acc,\n","    A23_fa_train, \n","    [A23_fa_test, A3_mp_test, A2_fa_test],\n","    EPOCHS,\n","    cuda_dev)\n","embeds_a23cc_post = gather_embeddings_MC(a23_mc_cc_model, [A3_a, A3_b, A2_a, A2_b])"]},{"cell_type":"code","source":["a23_mc_cc_per_q_accs   = per_q_ml_acc(a23_mc_cc_model, dl_perqacc)\n","a23_mc_cc_per_ctx_accs = per_c_ml_acc(a23_mc_cc_model, dls_ml_contexts)"],"metadata":{"id":"UAkH5Ok29xJO","executionInfo":{"status":"ok","timestamp":1680311074340,"user_tz":240,"elapsed":12484,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"execution_count":204,"outputs":[]},{"cell_type":"code","source":["print(\"Content-Context\")\n","for q, acc in zip(question_ids, a23_mc_cc_per_q_accs.tolist()):\n","  print(f\"  q {q:2d},  acc: {acc:0.8f}\")\n","print(\"---\")\n","for ctx, acc in a23_mc_cc_per_ctx_accs:\n","  print(f\"ctx {ctx}, acc: {acc:0.8f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cjiq0ec-Rex","executionInfo":{"status":"ok","timestamp":1680311077132,"user_tz":240,"elapsed":428,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}},"outputId":"32c6231a-d112-41cd-f0af-f0eeaa7ecfce"},"execution_count":205,"outputs":[{"output_type":"stream","name":"stdout","text":["Content-Context\n","  q  5,  acc: 0.66008770\n","  q  6,  acc: 0.89364034\n","  q  7,  acc: 0.91885966\n","  q  8,  acc: 0.97697371\n","  q 10,  acc: 0.98355263\n","  q 11,  acc: 0.89035088\n","  q 12,  acc: 0.81140351\n","---\n","ctx 205, acc: 0.85612245\n","ctx 206, acc: 0.87868481\n","ctx 207, acc: 0.84918833\n","ctx 208, acc: 0.87500000\n","ctx 209, acc: 0.88541669\n","ctx 211, acc: 0.88340337\n"]}]},{"cell_type":"markdown","metadata":{"id":"LUw8Wh1NDLef"},"source":["### Binary Models\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oW3GllDrDZWw"},"outputs":[],"source":["A23_mp_train, A23_mp_test = create_dataloaders_bin(content_a3a2_mp, \n","                                             0.2,\n","                                             balance=True)\n","\n","A3_fa_train, A3_fa_test = create_dataloaders_bin(content_a3_fa, \n","                                             0.2,\n","                                             balance=False)\n","A2_fa_train, A2_fa_test = create_dataloaders_bin(content_a2_fa, \n","                                             0.2, \n","                                             balance=False)\n","\n","test_batches = [\n","  A23_mp_test,\n","  A3_fa_test,\n","  A2_fa_test\n","]\n","\n","# For generating the 'full set' of tSNE embeddings. \n","A3_a, A3_b = create_dataloaders_bin(content_a3_majplus, \n","                                0.5,\n","                                balance=False)\n","A2_a, A2_b = create_dataloaders_bin(content_a2_fa, \n","                                0.5,\n","                                balance=False)"]},{"cell_type":"code","source":["# We need dataloaders for the per-question and per-context evaluation.\n","dls_questions = []\n","for g_id, g_df in content_a3a2_mp.groupby(by=['question_id']):\n","  g_dl = create_dataloaders_bin(g_df, 0)\n","  dls_questions.append([g_id, g_dl])\n","\n","dls_contexts = []\n","for g_id, g_df in content_a3a2_mp.groupby(by=['context_id']):\n","  g_dl = create_dataloaders_bin(g_df, 0)\n","  dls_contexts.append([g_id, g_dl])"],"metadata":{"id":"e_gw0Navtuch"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_yte4uxRIPh"},"outputs":[],"source":["def binary_acc(y_pred, y_test):\n","  y_pred_tag = torch.round(torch.sigmoid(y_pred))\n","  correct_results_sum = (y_pred_tag == y_test).sum().float()\n","  acc = correct_results_sum/float(y_test.shape[0])\n","  return acc"]},{"cell_type":"code","source":["def per_x_bin_acc(model, dataloaders):\n","  model.eval()\n","  accs = []\n","  for id, dl in dataloaders:\n","    batch_accs = []\n","    for batch in dl:\n","      batch = tuple(t.to(cuda_dev) for t in batch)\n","      labels = batch[-1]\n","      batch_accs.append(binary_acc(model(batch), labels).item())\n","    accs.append([id, sum(batch_accs)/float(len(batch_accs))])\n","  return accs"],"metadata":{"id":"cic1jEST-iiQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Js6BLq9CHFNh"},"source":["#### CQ-Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["94ec6efe00e34a6ab6218defc1bade35","5f40860fbc77437d9fa9b2a2555f8989","ccf9b73f4c284f38ac673649e9fbd350","cf0588acdc684fa68d8ef8c7d8fad147","5a55d90e6dab49ca93ee1b0b965a9224","136a2b1164044b10b96adcf4248b974a","c373d2730260449eb1b4adc2320dfaaa","fd0229d2640e437a8a0d8c7a72dce2fa","2a7c8fd578bd4530a588a6c95cb4b062","a64abc21e79e4d2ba31b1622bd00d5a6","5748632a61db4cb2a7227548e1eb4a86"]},"id":"z9VrskTbHFNu","executionInfo":{"status":"ok","timestamp":1680295741669,"user_tz":240,"elapsed":4144744,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}},"outputId":"d24acf07-3847-4e8b-b347-87d05f448def"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ec6efe00e34a6ab6218defc1bade35"}},"metadata":{}}],"source":["a23_cq_model = AQAM_B_CQ().to(cuda_dev)\n","\n","embeds_a23cq_pre = gather_embeddings_bin(a23_cq_model, [A3_a, A3_b, A2_a, A2_b])\n","train_a23cq, test_a23cq, a23_cq_model = train_and_evaluate(\n","    a23_cq_model,\n","    binary_acc,\n","    A23_mp_train, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)\n","embeds_a23cq_post = gather_embeddings_bin(a23_cq_model, [A3_a, A3_b, A2_a, A2_b])"]},{"cell_type":"code","source":["a23_bin_cq_per_q_accs   = per_x_bin_acc(a23_cq_model, dls_questions)\n","a23_bin_cq_per_ctx_accs = per_x_bin_acc(a23_cq_model, dls_contexts)"],"metadata":{"id":"Egxbd5TJ_MTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Content-Question\")\n","for q, acc in a23_bin_cq_per_q_accs:\n","  print(f\"q   {q:2d},  acc: {acc:0.3f}\")\n","print(\"---\")\n","for ctx, acc in a23_bin_cq_per_ctx_accs:\n","  print(f\"ctx {ctx}, acc: {acc:0.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GlTdHwVsBI8p","executionInfo":{"status":"ok","timestamp":1680295810163,"user_tz":240,"elapsed":14,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}},"outputId":"6123ecb3-93f1-4916-eb18-87769d235435"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Content-Question\n","q    5,  acc: 0.902\n","q    6,  acc: 0.841\n","q    7,  acc: 0.920\n","q    8,  acc: 0.967\n","q   10,  acc: 0.977\n","q   11,  acc: 0.821\n","q   12,  acc: 0.804\n","---\n","ctx 205, acc: 0.907\n","ctx 206, acc: 0.885\n","ctx 207, acc: 0.945\n","ctx 208, acc: 0.955\n","ctx 209, acc: 0.912\n","ctx 211, acc: 0.873\n"]}]},{"cell_type":"markdown","metadata":{"id":"opCy4niXHFzO"},"source":["#### CCQ-Model"]},{"cell_type":"code","execution_count":160,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["3a2f659cccfd4da5aa2b6e5a019b2e31","6251c034366d481e891a859322407e86","3ba4fc1e0b794cdf9dff6ab1a1f2ca30","d5f81add61714d8ca5da1a1b62de3c31","f8d05908a00442358966f77cd6e914de","2aca3f954a164f1fbdbc3872cbc1dac3","70ec32bcd5f14148b38d6c4441b91d20","3986baa0febe44479d0d138b872ba9e1","688defb9ef16442c90d74b41e97c22bd","1aa3c9e2bd78452f91c67c5930805b89","073f37f81b9640fb854953d972f17dbe"]},"id":"xYUa1B_JHFzV","outputId":"07b409b7-6c82-4291-ad6f-46d2366dacae","executionInfo":{"status":"ok","timestamp":1680301915444,"user_tz":240,"elapsed":1354431,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a2f659cccfd4da5aa2b6e5a019b2e31","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["a23_ccq_model = AQAM_B_CCQ().to(cuda_dev)\n","\n","embeds_a23ccq_pre = gather_embeddings_bin(a23_ccq_model, [A3_a, A3_b, A2_a, A2_b])\n","train_a23ccq, test_a23ccq, a23_ccq_model = train_and_evaluate(\n","    a23_ccq_model,\n","    binary_acc,\n","    A23_mp_train, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)\n","embeds_a23ccq_post = gather_embeddings_bin(a23_ccq_model, [A3_a, A3_b, A2_a, A2_b])"]},{"cell_type":"code","source":["a23_bin_ccq_per_q_accs   = per_x_bin_acc(a23_ccq_model, dls_questions)\n","a23_bin_ccq_per_ctx_accs = per_x_bin_acc(a23_ccq_model, dls_contexts)"],"metadata":{"id":"AXBfSpnqB1k-","executionInfo":{"status":"ok","timestamp":1680302018068,"user_tz":240,"elapsed":102621,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"execution_count":161,"outputs":[]},{"cell_type":"code","source":["print(\"Content-Context-Question\")\n","for q, acc in a23_bin_ccq_per_q_accs:\n","  print(f\"q   {q:2d},  acc: {acc:0.3f}\")\n","print(\"---\")\n","for ctx, acc in a23_bin_ccq_per_ctx_accs:\n","  print(f\"ctx {ctx}, acc: {acc:0.3f}\")"],"metadata":{"id":"Dzn6xZn1B8Lk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680302018091,"user_tz":240,"elapsed":0,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}},"outputId":"9f18daac-7e05-4464-9940-6a2fdb3d87dc"},"execution_count":162,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Content-Context-Question\n","q    5,  acc: 0.884\n","q    6,  acc: 0.900\n","q    7,  acc: 0.939\n","q    8,  acc: 0.967\n","q   10,  acc: 0.977\n","q   11,  acc: 0.865\n","q   12,  acc: 0.810\n","---\n","ctx 205, acc: 0.930\n","ctx 206, acc: 0.909\n","ctx 207, acc: 0.963\n","ctx 208, acc: 0.955\n","ctx 209, acc: 0.925\n","ctx 211, acc: 0.870\n"]}]},{"cell_type":"markdown","metadata":{"id":"LYPWPFLrKgF2"},"source":["# Results"]},{"cell_type":"code","execution_count":163,"metadata":{"id":"zFmDNG_UheNS","executionInfo":{"status":"ok","timestamp":1680302018072,"user_tz":240,"elapsed":2,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"outputs":[],"source":["# Prepended to image and data pickles before uploading to GCS. \n","RESULTS_NOTE = \"05_samesettings_saving_perqctx_accs\""]},{"cell_type":"markdown","source":["## Save Raw Result Data"],"metadata":{"id":"xlT8xii6CFqE"}},{"cell_type":"code","source":["per_qctx_accs = [\n","    [\"context-only\",     a23_mc_c_per_q_accs, a23_mc_c_per_ctx_accs],\n","    [\"context-content\",  a23_mc_cc_per_q_accs, a23_mc_cc_per_ctx_accs],\n","    [\"content-question\", a23_bin_cq_per_q_accs, a23_bin_cq_per_ctx_accs],\n","    [\"content-context-question\", a23_bin_ccq_per_q_accs, a23_bin_ccq_per_ctx_accs],\n","]"],"metadata":{"id":"SrUZCc9yCCsn","executionInfo":{"status":"ok","timestamp":1680302018074,"user_tz":240,"elapsed":3,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"execution_count":164,"outputs":[]},{"cell_type":"code","source":["results_obj_fn = f\"{RESULTS_NOTE}_perqctx_results_obj.p\"\n","with open(results_obj_fn, 'wb') as f:\n","  pickle.dump(per_qctx_accs, f)\n","\n","upload_file_to_gcs(results_obj_fn, f\"{RESULTS_GCS_DIR}/{results_obj_fn}\")"],"metadata":{"id":"ls8kI9OFC9sg","executionInfo":{"status":"ok","timestamp":1680302020484,"user_tz":240,"elapsed":2413,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"execution_count":165,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pqj-ZyDqNUQ9"},"source":["## tSNE Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6GyuO34Nc8Y"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","PERPLEX = 20"]},{"cell_type":"markdown","metadata":{"id":"UvJWbHB_KpQi"},"source":["### Multi-Label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFmu4VroNx2O"},"outputs":[],"source":["a23_cnt_pre, _,  a23_cnt_agree_pre  = embeds_a23c_pre\n","a23_cnt_post, _, a23_cnt_agree_post = embeds_a23c_post\n","\n","_, a23_cntctx_pre,  a23_cntctx_agree_pre  = embeds_a23cc_pre\n","_, a23_cntctx_post, a23_cntctx_agree_post = embeds_a23cc_post"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGxJTp9iPbRS"},"outputs":[],"source":["lde_c_pre = TSNE(n_components=2, \n","                 learning_rate='auto', \n","                 init='random', \n","                 perplexity=PERPLEX).fit_transform(a23_cnt_pre)\n","\n","lde_c_post = TSNE(n_components=2, \n","                  learning_rate='auto', \n","                  init='random', \n","                  perplexity=PERPLEX).fit_transform(a23_cnt_post)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJ0wAtgLP6uj"},"outputs":[],"source":["lde_cc_pre = TSNE(n_components=2, \n","                 learning_rate='auto', \n","                 init='random', \n","                 perplexity=PERPLEX).fit_transform(a23_cntctx_pre)\n","\n","lde_cc_post = TSNE(n_components=2, \n","                  learning_rate='auto', \n","                  init='random', \n","                  perplexity=PERPLEX).fit_transform(a23_cntctx_post)"]},{"cell_type":"markdown","metadata":{"id":"pCVeGJYNKyew"},"source":["### Binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbHWGKVvKxux"},"outputs":[],"source":["_, _, a23_cntq_pre, _ , a23_cntq_agree_pre  = embeds_a23cq_pre\n","_, _, a23_cntq_post, _, a23_cntq_agree_post = embeds_a23cq_post\n","\n","_, _, _, a23_cntctxq_pre , a23_cntctxq_agree_pre  = embeds_a23ccq_pre\n","_, _, _, a23_cntctxq_post, a23_cntctxq_agree_post = embeds_a23ccq_post"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTGUNXAlP62B"},"outputs":[],"source":["lde_cq_pre = TSNE(n_components=2, \n","                 learning_rate='auto', \n","                 init='random', \n","                 perplexity=PERPLEX).fit_transform(a23_cntq_pre)\n","\n","lde_cq_post = TSNE(n_components=2, \n","                  learning_rate='auto', \n","                  init='random', \n","                  perplexity=PERPLEX).fit_transform(a23_cntq_post)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_Cwhs41P6_I"},"outputs":[],"source":["lde_ccq_pre = TSNE(n_components=2, \n","                 learning_rate='auto', \n","                 init='random', \n","                 perplexity=PERPLEX).fit_transform(a23_cntctxq_pre)\n","\n","lde_ccq_post = TSNE(n_components=2, \n","                  learning_rate='auto', \n","                  init='random', \n","                  perplexity=PERPLEX).fit_transform(a23_cntctxq_post)"]},{"cell_type":"markdown","metadata":{"id":"8xq-nOCDNd8F"},"source":["## Plot and Save"]},{"cell_type":"code","execution_count":173,"metadata":{"id":"Ut5v_dXQGJMv","executionInfo":{"status":"ok","timestamp":1680306933777,"user_tz":240,"elapsed":1,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"outputs":[],"source":["results_object = [\n","    [lde_c_pre,   lde_c_post,   a23_cnt_agree_pre,    a23_cnt_agree_post,    train_a23c,   test_a23c,   \"content only\"],\n","    [lde_cc_pre,  lde_cc_post,  a23_cntctx_agree_pre, a23_cntctx_agree_post, train_a23cc,  test_a23cc,  \"content-context\"],\n","    [lde_cq_pre,  lde_cq_post,  a23_cntq_agree_pre,    a23_cntq_agree_post,    train_a23cq,  test_a23cq,  \"content-question\"],\n","    [lde_ccq_pre, lde_ccq_post, a23_cntctxq_agree_pre, a23_cntctxq_agree_post, train_a23ccq, test_a23ccq, \"content-context-question\"]\n","]\n","\n","results_obj_fn = f\"{RESULTS_NOTE}_raw_results_obj.p\"\n","with open(results_obj_fn, 'wb') as f:\n","  pickle.dump(results_object, f)\n","\n","upload_file_to_gcs(results_obj_fn, f\"{RESULTS_GCS_DIR}/{results_obj_fn}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWTfLg3sNgu3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","from matplotlib import colormaps\n","%matplotlib notebook\n","%matplotlib inline\n","\n","MIN_Y_ACC = 0.60\n","MAX_Y_ACC = 1.0\n","Y_ACC_LIM = (MIN_Y_ACC, MAX_Y_ACC)\n","\n","CUTOFF = 50\n","\n","fig, axis = plt.subplots(4, 3) # 4 Rows, with 3 plots each. \n","\n","fig.set_figwidth(15)\n","fig.set_figheight(15)\n","\n","fig.tight_layout(pad=3)\n","\n","acc_labels = [\"A23M+ Train\", \"A23M+ Test\", \"A3M+ Test\", \"A2 FA Test\"]\n","acc_colors = [\"tab:red\", \"tab:orange\", \"tab:green\", \"tab:blue\"]\n","\n","def plot_row(lde_pre, lde_post, lde_pre_agree, lde_post_agree, train_acc, \n","             test_accs, data_label, row_n, colors, cpu=False):\n","  x_pre,  y_pre  = np.split(lde_pre, 2, axis=1)\n","  x_post, y_post = np.split(lde_post, 2, axis=1)\n","\n","  # These should all use the 7-value color map. \n","  # pre-finetuning tsne scatterplot\n","  axis[row_n, 0].scatter(x_pre, y_pre, c=lde_pre_agree, cmap=colors, marker=\".\")\n","  axis[row_n, 0].set_title(f\"{data_label} embeddings pre\")\n","\n","  # post-finetuning tsne scatterplot\n","  axis[row_n, 1].scatter(x_post, y_post,  c=lde_post_agree, cmap=colors, marker=\".\")\n","  axis[row_n, 1].set_title(f\"{data_label} embeddings post\")\n","\n","  # training and test set acc's by epoch.\n","  acc_data_set = [train_acc]\n","  acc_data_set.extend(test_accs)\n","  axis[row_n, 2].set(ylim=Y_ACC_LIM)\n","  for l, test, color in zip(acc_labels, acc_data_set, acc_colors):\n","    if cpu: \n","      test = [i.cpu() for i in test]\n","    axis[row_n, 2].plot(test[:CUTOFF], label=l, color=color)\n","\n","  axis[row_n, 2].set_title(f\"{data_label} acc\")\n","  axis[row_n, 2].legend()\n","\n","# MC Rows\n","plot_row(lde_c_pre, lde_c_post, a23_cnt_agree_pre, a23_cnt_agree_post, \n","         train_a23c, test_a23c, \"content only\", 0, colors=colormaps['Blues'])\n","\n","plot_row(lde_cc_pre, lde_cc_post, a23_cntctx_agree_pre, a23_cntctx_agree_post,  \n","         train_a23cc, test_a23cc, \"content-context\", 1, colors=colormaps['Blues'])\n","\n","# Binary Rows\n","plot_row(lde_cq_pre, lde_cq_post, a23_cntq_agree_pre, a23_cntq_agree_post,  \n","         train_a23cq, test_a23cq, \"content-question\", 2, \n","         ListedColormap([\"red\", \"blue\"]), cpu=True)\n","plot_row(lde_ccq_pre, lde_ccq_post, a23_cntctxq_agree_pre, a23_cntctxq_agree_post, \n","         train_a23ccq, test_a23ccq, \"content-context-question\", 3, \n","         ListedColormap([\"red\", \"blue\"]), cpu=True)\n","\n","plt_fn = f\"{RESULTS_NOTE}_full_comparison_v2.png\"\n","plt.savefig(plt_fn)\n","plt.show()\n","\n","upload_file_to_gcs(plt_fn, f\"{RESULTS_GCS_DIR}/{RESULTS_NOTE}_full_comparison_v2.png\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["E2UNaoi3uMlP","AbeAB8Hokb-2","FC4VN9OX7ASA","TBwBOc1LJWSS","YEniTxLUPL-K","Df9iIZyhkb--","kNQBRgQkkb_B","noOpZRP8yXpr","Ee2A1EDh0Zgd","x6sTgOXpAQRP","TbrkrVp9unHC","h6ilu9ax8bze","4F9mZnh3xKZF","HQROFGFECBfT","2kxrcLbaCRmg","LYPWPFLrKgF2","Pqj-ZyDqNUQ9"],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPnaX7rRWp9DW3fQs6yFw52"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"43bc96a1f53647d8b1a2c54cb708ab02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df14f7415fbe4db59fee86541a0ee42e","IPY_MODEL_506417c0d95e420e93769e821dadc5da","IPY_MODEL_b422d514d994460fbbf5b594ab8ee9bb"],"layout":"IPY_MODEL_78ea5f30e06c4f7fa6427f61ee363dab"}},"df14f7415fbe4db59fee86541a0ee42e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2548eeedc295474aa327177691adb79d","placeholder":"​","style":"IPY_MODEL_ec977e2b733e44739d761b73fd1dc40e","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"506417c0d95e420e93769e821dadc5da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe7df1ac2fd842e7a960991dd2453fbc","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7da463b24577492c94ec4db66daa7f13","value":231508}},"b422d514d994460fbbf5b594ab8ee9bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00a9d51db6d24f5d896f2bb72cf7981e","placeholder":"​","style":"IPY_MODEL_febd3c4ce4944bb8801a2262d4876591","value":" 232k/232k [00:00&lt;00:00, 408kB/s]"}},"78ea5f30e06c4f7fa6427f61ee363dab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2548eeedc295474aa327177691adb79d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec977e2b733e44739d761b73fd1dc40e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe7df1ac2fd842e7a960991dd2453fbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7da463b24577492c94ec4db66daa7f13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00a9d51db6d24f5d896f2bb72cf7981e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"febd3c4ce4944bb8801a2262d4876591":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c8dbc17bcdb4aec8d6072bb7b58edd7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_032e7ccfac8b43daa781050c258052ad","IPY_MODEL_868a6b1ff59a450e9069449e04ca04c5","IPY_MODEL_b535cd62d1514c6c9aa4466fb20e69ba"],"layout":"IPY_MODEL_5e1c661a0bbb434c9ef74aaca877605b"}},"032e7ccfac8b43daa781050c258052ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_181f583949c2460a915c4bfed55402f4","placeholder":"​","style":"IPY_MODEL_913d049a6ad24c19bf7becb01a9ec614","value":"Downloading (…)okenizer_config.json: 100%"}},"868a6b1ff59a450e9069449e04ca04c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b510c19f01c4b7ab36ff3350551b826","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1af7a797c122452da0d6fa697a7abb05","value":28}},"b535cd62d1514c6c9aa4466fb20e69ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d32c440a4b46460d919133ce2ae9b325","placeholder":"​","style":"IPY_MODEL_0caf4f0edc784bfcb2c85ffa831da4fc","value":" 28.0/28.0 [00:00&lt;00:00, 1.79kB/s]"}},"5e1c661a0bbb434c9ef74aaca877605b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"181f583949c2460a915c4bfed55402f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"913d049a6ad24c19bf7becb01a9ec614":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b510c19f01c4b7ab36ff3350551b826":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1af7a797c122452da0d6fa697a7abb05":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d32c440a4b46460d919133ce2ae9b325":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0caf4f0edc784bfcb2c85ffa831da4fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"404a91190070408faa663574782ca700":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cb934de44774180b405cfd92f4cbceb","IPY_MODEL_bcb05d0e8e7f49ea981688851811a19a","IPY_MODEL_6181001185634d7c81f1fbd355187505"],"layout":"IPY_MODEL_a7b2d8d3160d455ea44c27224918197e"}},"3cb934de44774180b405cfd92f4cbceb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bb8aa0d808042f5a5faaf91328c2d61","placeholder":"​","style":"IPY_MODEL_281c293c85394623835f62da3fa4235d","value":"Downloading (…)lve/main/config.json: 100%"}},"bcb05d0e8e7f49ea981688851811a19a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e68665a1844b4d23899bc4c03ea65e5a","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13b029a1b54844ae9d5eca208a37f70d","value":483}},"6181001185634d7c81f1fbd355187505":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d209895c968847bebdb4c7ab9fb6392d","placeholder":"​","style":"IPY_MODEL_a74a2f8810b345da83ad69ec25502a92","value":" 483/483 [00:00&lt;00:00, 36.4kB/s]"}},"a7b2d8d3160d455ea44c27224918197e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb8aa0d808042f5a5faaf91328c2d61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"281c293c85394623835f62da3fa4235d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e68665a1844b4d23899bc4c03ea65e5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13b029a1b54844ae9d5eca208a37f70d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d209895c968847bebdb4c7ab9fb6392d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a74a2f8810b345da83ad69ec25502a92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94ec6efe00e34a6ab6218defc1bade35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f40860fbc77437d9fa9b2a2555f8989","IPY_MODEL_ccf9b73f4c284f38ac673649e9fbd350","IPY_MODEL_cf0588acdc684fa68d8ef8c7d8fad147"],"layout":"IPY_MODEL_5a55d90e6dab49ca93ee1b0b965a9224"}},"5f40860fbc77437d9fa9b2a2555f8989":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_136a2b1164044b10b96adcf4248b974a","placeholder":"​","style":"IPY_MODEL_c373d2730260449eb1b4adc2320dfaaa","value":"100%"}},"ccf9b73f4c284f38ac673649e9fbd350":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd0229d2640e437a8a0d8c7a72dce2fa","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a7c8fd578bd4530a588a6c95cb4b062","value":25}},"cf0588acdc684fa68d8ef8c7d8fad147":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a64abc21e79e4d2ba31b1622bd00d5a6","placeholder":"​","style":"IPY_MODEL_5748632a61db4cb2a7227548e1eb4a86","value":" 25/25 [1:06:11&lt;00:00, 159.10s/epoch]"}},"5a55d90e6dab49ca93ee1b0b965a9224":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"136a2b1164044b10b96adcf4248b974a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c373d2730260449eb1b4adc2320dfaaa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd0229d2640e437a8a0d8c7a72dce2fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a7c8fd578bd4530a588a6c95cb4b062":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a64abc21e79e4d2ba31b1622bd00d5a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5748632a61db4cb2a7227548e1eb4a86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a2f659cccfd4da5aa2b6e5a019b2e31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6251c034366d481e891a859322407e86","IPY_MODEL_3ba4fc1e0b794cdf9dff6ab1a1f2ca30","IPY_MODEL_d5f81add61714d8ca5da1a1b62de3c31"],"layout":"IPY_MODEL_f8d05908a00442358966f77cd6e914de"}},"6251c034366d481e891a859322407e86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aca3f954a164f1fbdbc3872cbc1dac3","placeholder":"​","style":"IPY_MODEL_70ec32bcd5f14148b38d6c4441b91d20","value":"100%"}},"3ba4fc1e0b794cdf9dff6ab1a1f2ca30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3986baa0febe44479d0d138b872ba9e1","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_688defb9ef16442c90d74b41e97c22bd","value":25}},"d5f81add61714d8ca5da1a1b62de3c31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aa3c9e2bd78452f91c67c5930805b89","placeholder":"​","style":"IPY_MODEL_073f37f81b9640fb854953d972f17dbe","value":" 25/25 [1:38:18&lt;00:00, 235.89s/epoch]"}},"f8d05908a00442358966f77cd6e914de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aca3f954a164f1fbdbc3872cbc1dac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70ec32bcd5f14148b38d6c4441b91d20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3986baa0febe44479d0d138b872ba9e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"688defb9ef16442c90d74b41e97c22bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1aa3c9e2bd78452f91c67c5930805b89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"073f37f81b9640fb854953d972f17dbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82d78dde5efa455a950ca1913d7c231e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb77d9784ca74abdb5cb75f6e8e55f71","IPY_MODEL_2a48aa7d3c714ab7b85c94ef5d8625c5","IPY_MODEL_f640a59bacd94604a9b1516fa62b51ff"],"layout":"IPY_MODEL_60fedf701b9943b3ac7eec7d892c3cb6"}},"eb77d9784ca74abdb5cb75f6e8e55f71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efdcacae65cf43d1b62e10d0a5ba79a3","placeholder":"​","style":"IPY_MODEL_9e2241817da24b009e15cc6b3c9098c4","value":"100%"}},"2a48aa7d3c714ab7b85c94ef5d8625c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78451b1aca5d479797e9df5bfe4d6dcc","max":23,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e78745eb9294cba891a2da3c70132df","value":23}},"f640a59bacd94604a9b1516fa62b51ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38654dc5ab9c49d8854e1e6affa31fba","placeholder":"​","style":"IPY_MODEL_190ceaa536d54e03a90c0a3b935d121e","value":" 23/23 [03:17&lt;00:00,  8.83s/epoch]"}},"60fedf701b9943b3ac7eec7d892c3cb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efdcacae65cf43d1b62e10d0a5ba79a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e2241817da24b009e15cc6b3c9098c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78451b1aca5d479797e9df5bfe4d6dcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e78745eb9294cba891a2da3c70132df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38654dc5ab9c49d8854e1e6affa31fba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"190ceaa536d54e03a90c0a3b935d121e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1192de6859bd4daebbbb6c6704713989":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abbf21510fd64d90b9dad93a6feb5e8e","IPY_MODEL_3f83fca3703b4a5ab1bff68fe3ddca93","IPY_MODEL_38197922f4ae4e559d4a5fb9a20399c4"],"layout":"IPY_MODEL_d2f37c3b7fa047ad9f6a041b3c36366d"}},"abbf21510fd64d90b9dad93a6feb5e8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ab70251a855413f837835d27dd7da05","placeholder":"​","style":"IPY_MODEL_1cb1bf9ac9f84680a99241d89a0fba1c","value":"100%"}},"3f83fca3703b4a5ab1bff68fe3ddca93":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eea286a034aa461eb813768d506aff76","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e07bc32a01d94fadad09935a78f87516","value":25}},"38197922f4ae4e559d4a5fb9a20399c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec1f3ccdf9a940288e3dfc1006fb0eb9","placeholder":"​","style":"IPY_MODEL_28ae974309f840409a6e40a9dc5c3d8e","value":" 25/25 [07:10&lt;00:00, 17.69s/epoch]"}},"d2f37c3b7fa047ad9f6a041b3c36366d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ab70251a855413f837835d27dd7da05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cb1bf9ac9f84680a99241d89a0fba1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eea286a034aa461eb813768d506aff76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e07bc32a01d94fadad09935a78f87516":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec1f3ccdf9a940288e3dfc1006fb0eb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ae974309f840409a6e40a9dc5c3d8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}