{"cells":[{"cell_type":"markdown","metadata":{"id":"9UZ93zQklxZk"},"source":["# Overview\n","To support answering RQ3, we want to look at the following types of model/approach to improving the quality of weakly labeled data.\n","\n","We have two sets of data, which will each be split into train and validation/test sets; A3_Maj+ data (3 annotators, 2/3 or 3/3 annotators agree), and A2_FullAgreement (2 annotators, 2/2 annotators agree). So in the end we have 4 'chunks' of data; A2_fa_train, A2_fa_test, A3_maj+_train, A3_maj+_test. \n","\n","* Baseline - No additional A2 training data\n","  - Train for X epochs - On A3_Maj+_train\n","  - Evaluate on test sets\n","  - Train for additional X epochs - On A3_Maj+_train\n","  - Evaluate again. \n","  - Same as just running for 2*X epochs\n","* 'Full' Augmentation\n","  - Train for X epochs - On A3_Maj+_train\n","  - Evaluate on test sets\n","  - Train for X epochs - On A3_Maj+ AND A2_fa_train\n","  - Evaluate again.\n","* Filtered Augmentation\n","  - Train for X epochs - On A3_Maj+_train\n","  - Evaluate test sets\n","  - Use model to filter out any example in A2_fa_train that doesn't agree with trained mode, call the result A2_fa_filtered\n","  - Train for X epochs - On A3_Maj+ AND A2_fa_filtered\n","  - Evaluate again.\n"]},{"cell_type":"markdown","metadata":{"id":"AbeAB8Hokb-2"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"0fSMAjHpkb-4"},"source":["## Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNU4HLNZkb-5"},"outputs":[],"source":["RESULTS_GCS_DIR = \"results/AQA13\"\n","\n","GCS_APP_ID = \"aqa-research\"\n","GCS_BUCKET = \"dabi-aqa-data-00\"\n","\n","FN_QUESTIONS = \"questions_01.csv\"\n","FN_CONTEXTS  = \"contexts_01.csv\"\n","\n","FN_A2_RAW = \"a2_raw.csv\"\n","FN_A2_FA  = \"a2_gs.csv\"\n","FN_A3_FA  = \"a3_gs.csv\"\n","FN_A3_MAJ = \"a3_con.csv\"\n","\n","MAX_SEQ_LEN = 128\n","\n","EPOCHS = 25\n","BATCH_SIZE = 16\n","TEST_FRAC = 0.1\n","\n","RAND_SEED = 4246"]},{"cell_type":"markdown","metadata":{"id":"FC4VN9OX7ASA"},"source":["## Pippin'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5778,"status":"ok","timestamp":1679025588822,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"MjPN6S5z6_7_","outputId":"1f0d93dc-1e07-47e6-f00f-42e698bbd5d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.4\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10564,"status":"ok","timestamp":1679025599373,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"F0uhaIcV7LtL","outputId":"564d9177-fb02-420d-e1dd-8100247b13da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.27.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"ybu8iEZskb-3"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiAEt7P6kb-3"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import torch\n","import random\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from tqdm.notebook import tqdm\n","import pickle\n","\n","import transformers as ppb\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","\n","from torch.optim import SGD\n","from torchmetrics.classification import MultilabelAccuracy"]},{"cell_type":"markdown","metadata":{"id":"glKP3nrhkb-6"},"source":["## GCS Auth - Input Needed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11079,"status":"ok","timestamp":1679025615647,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"xwxl9-b4kb-7","outputId":"7003e18d-5962-49fe-f24d-76df5ed60e5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Go to the following link in your browser:\n","\n","    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=j0tfLBr0Bj27TV4wm42V7sAZh3Et5p&prompt=consent&access_type=offline&code_challenge=YGlORI2CibIfoBMwC6_yL11lA8huq1XL1AXgBCVfO5A&code_challenge_method=S256\n","\n","Enter authorization code: 4/0AWtgzh5XS_mRw609cGHlWovVjyi9ShAvFDw365HinrMcR4Gyx0UYhTvo43My17euW9SuNQ\n","\n","You are now logged in as [willpowe@gmail.com].\n","Your current project is [None].  You can change this setting by running:\n","  $ gcloud config set project PROJECT_ID\n"]}],"source":["!gcloud auth login --launch-browser"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1443,"status":"ok","timestamp":1679025617076,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"SEQdfjlikb-7","outputId":"c89e3028-57bb-4686-c4ee-a5558b3bc464"},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated property [core/project].\n"]}],"source":["!gcloud config set project {GCS_APP_ID}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x_hDs2gkb-8"},"outputs":[],"source":["def download_file_from_gcs(src_fn, dest_fn):\n","  dest = f\"/content/{dest_fn}\"\n","  dl_command = f\"gsutil -m cp gs://{GCS_BUCKET}/{src_fn} {dest}\"\n","  os.system(dl_command)\n","\n","def upload_file_to_gcs(src_fn, dest_fn):\n","  dest_url = \"{}/{}\".format(GCS_BUCKET, dest_fn)\n","  ul_command = \"gsutil -m cp {} gs://{}\".format(src_fn, dest_url)\n","  os.system(ul_command)"]},{"cell_type":"markdown","metadata":{"id":"TBwBOc1LJWSS"},"source":["## Random Seeds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1679025617078,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"HNrkkYVqJV62","outputId":"cdb82405-b97d-4d2f-f776-01499dc6bc9e"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f4158ce1570>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["random.seed(RAND_SEED)\n","np.random.seed(RAND_SEED)\n","torch.manual_seed(RAND_SEED)"]},{"cell_type":"markdown","metadata":{"id":"YEniTxLUPL-K"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"Df9iIZyhkb--"},"source":["## Raw Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOIG4PnXkb-_"},"outputs":[],"source":["download_file_from_gcs(FN_QUESTIONS, FN_QUESTIONS)\n","download_file_from_gcs(FN_CONTEXTS, FN_CONTEXTS)\n","\n","download_file_from_gcs(FN_A2_RAW, FN_A2_RAW)\n","download_file_from_gcs(FN_A2_FA, FN_A2_FA)\n","download_file_from_gcs(FN_A3_FA, FN_A3_FA)\n","download_file_from_gcs(FN_A3_MAJ, FN_A3_MAJ)"]},{"cell_type":"markdown","metadata":{"id":"CnejujZg8M8q"},"source":["## Dataframes"]},{"cell_type":"markdown","metadata":{"id":"pxorHwUB8Ro-"},"source":["### Question/Context Dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PryV0dbr8SOA"},"outputs":[],"source":["questions = pd.read_csv(FN_QUESTIONS)\n","contexts  = pd.read_csv(FN_CONTEXTS)"]},{"cell_type":"markdown","metadata":{"id":"zUUz0lb2yLAg"},"source":["### Binary View Dataframes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuLRs3tXkb_C"},"outputs":[],"source":["content_a2_raw = pd.read_csv(FN_A2_RAW)\n","content_a2_fa  = pd.read_csv(FN_A2_FA)\n","content_a3_fa  = pd.read_csv(FN_A3_FA)\n","content_a3_maj = pd.read_csv(FN_A3_MAJ)\n","\n","# Could also try this with using the 3's as 0's\n","content_a2_raw = content_a2_raw[content_a2_raw['answer'] != 3] \n","content_a2_fa  = content_a2_fa[content_a2_fa['answer'] != 3] \n","content_a3_fa  = content_a3_fa[content_a3_fa['answer'] != 3] \n","content_a3_maj = content_a3_maj[content_a3_maj['answer'] != 3] \n","\n","content_a3_majplus = pd.concat([content_a3_fa, content_a3_maj])\n","content_a3a2_mp    = pd.concat([content_a3_fa, content_a3_maj, content_a2_fa])"]},{"cell_type":"markdown","metadata":{"id":"7CQLt7Gb717p"},"source":["### Multi-label View Dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cf7EXtpPZxC4"},"outputs":[],"source":["question_ids = content_a3a2_mp['question_id'].unique()\n","\n","# Data for Multi-Label Classification Models.\n","df_a2fa_mc   = content_a2_fa.groupby(by=['context_id', 'comment_id'])\n","df_a3mp_mc   = content_a3_majplus.groupby(by=['context_id', 'comment_id'])\n","df_a3a2mp_mc = content_a3a2_mp.groupby(by=['context_id', 'comment_id'])"]},{"cell_type":"markdown","metadata":{"id":"kNQBRgQkkb_B"},"source":["## LLM Initalization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357,"referenced_widgets":["65212ebbc93f4cd0afb91eab9a8fa485","55fe9680d3c7443aad70578fa39e17e5","37d8840e6ffc4bb0a575924bef0f47db"]},"executionInfo":{"elapsed":6755,"status":"ok","timestamp":1679025637759,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"wrotnPAHkb_C","outputId":"e9eee12e-f291-45f2-bc56-e2b8f565353b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65212ebbc93f4cd0afb91eab9a8fa485","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55fe9680d3c7443aad70578fa39e17e5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37d8840e6ffc4bb0a575924bef0f47db","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# For DistilBERT:\n","model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n","\n","## Want BERT instead of distilBERT? Uncomment the following line:\n","#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","\n","# TODO - figure out the above for the Q/A model. Might need a different\n","#        enough implementation/approach that it warrents a dif script. \n","#        It takes a masked input for the Q and options for multiple choice? idk. \n","\n","# Load pretrained model/tokenizer\n","tokenizer  = tokenizer_class.from_pretrained(pretrained_weights, truncate=True,)"]},{"cell_type":"markdown","metadata":{"id":"noOpZRP8yXpr"},"source":["## Question/Context Embedding Dicts"]},{"cell_type":"markdown","metadata":{"id":"IOoeD1gRUkyE"},"source":["Used later to build the individual training examples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad4YwoEX2otO"},"outputs":[],"source":["def build_ts_am_map(df_in, text_col_name, id_col):\n","  ids   = df_in[id_col].values\n","  sents = df_in[text_col_name].values\n","\n","  token_seqs = []\n","  attn_masks = []\n","  id_2_tsam = dict()\n","  for id, sent in zip(ids, sents):\n","    enc_dict = tokenizer.encode_plus(\n","      sent,\n","      add_special_tokens = False,\n","      max_length = MAX_SEQ_LEN, \n","      pad_to_max_length = True,\n","      return_attention_mask = True,\n","      return_tensors = 'pt'\n","    )\n","    id_2_tsam[id] = [enc_dict['input_ids'], enc_dict['attention_mask']]\n","\n","  return id_2_tsam"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1679025637761,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"35ukWIxGuq_t","outputId":"f704fbb8-8832-4524-c5a2-42e574c61e82"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]}],"source":["map_q_2_ts_am   = build_ts_am_map(questions, 'question_text', 'question_id')\n","map_ctx_2_ts_am = build_ts_am_map(contexts, 'context_text', 'context_id')"]},{"cell_type":"markdown","metadata":{"id":"Ee2A1EDh0Zgd"},"source":["## Balancing - Over-Sampling Method\n","\n","Tweaked for MC."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8b-jBO3zaM9"},"outputs":[],"source":["# The ctx_idx will change depending on if this is being passed Binary or MC data\n","# 2 => MC\n","# ? => Binary\n","def max_size_oversampling_wr(examples, ctx_idx=2):\n","  question_counts = dict()\n","  context_counts  = dict()\n","  \n","  for ex in examples:\n","    ctx = ex[ctx_idx]\n","    if ctx not in context_counts:\n","      context_counts[ctx] = 0\n","    context_counts[ctx] += 1\n","\n","  context_examples    = { k: [] for k in context_counts}\n","  context_populations = { k: [] for k in context_counts}\n","\n","  for example in examples:\n","    ctx = example[ctx_idx] # This might change depending on shape of examples.\n","    context_populations[ctx].append(example)\n","\n","  max_size = 0\n","  for k in context_populations:\n","    size_pop = len(context_populations[k])\n","    if size_pop > max_size:\n","      max_size = size_pop\n","\n","  for k in context_examples:\n","    while len(context_examples[k]) < max_size:\n","      context_examples[k].append(random.sample(context_populations[k], 1)[0])\n","\n","  example_tuples_max_sampled = []\n","  for k in context_examples:\n","    for ex in context_examples[k]:\n","      example_tuples_max_sampled.append(ex)\n","  return example_tuples_max_sampled"]},{"cell_type":"markdown","metadata":{"id":"7EFQMMop0fyz"},"source":["## DataLoaders\n"]},{"cell_type":"markdown","metadata":{"id":"EiLNu1dR9rLf"},"source":["### Multi-Label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"at-9T_v0h60u"},"outputs":[],"source":["def create_example_tuples_mc(df_content, question_ids, balance=False):\n","  example_tuples = []\n","  \n","  for (context_id, comment_id), group in df_content:\n","    content = group['content_text'].values[0]\n","\n","    g_anss = []\n","    for q_id in question_ids:\n","      q_ans = group[group['question_id'] == q_id]['answer'].values\n","      if len(q_ans) > 0 and q_ans[0] != 3:\n","        g_anss.append(q_ans[0])\n","      else:\n","        g_anss.append(0)\n","\n","    example_tuples.append([\n","      map_ctx_2_ts_am[context_id],\n","      content,\n","      torch.reshape(torch.tensor(g_anss), (1, len(g_anss))),\n","      context_id\n","    ])\n","\n","  if balance:\n","    example_tuples = max_size_oversampling_wr(example_tuples, ctx_idx=2)\n","\n","  labels = [i[2] for i in example_tuples]\n","  iids_contexts  = []\n","  iids_contents  = []\n","  ams_contexts  = []\n","  ams_contents  = []\n","\n","  for [ts_ctx, am_ctx], sent, _, _ in example_tuples:\n","    iids_contexts.append(ts_ctx)\n","    ams_contexts.append(am_ctx)\n","\n","    enc_dict = tokenizer.encode_plus(\n","        sent,\n","        add_special_tokens = False,\n","        max_length = MAX_SEQ_LEN, \n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","    iids_contents.append(enc_dict['input_ids'])\n","    ams_contents.append(enc_dict['attention_mask'])\n","  \n","  iids_contexts  = torch.cat(iids_contexts,  dim=0)\n","  iids_contents  = torch.cat(iids_contents,  dim=0)\n","  ams_contexts  =  torch.cat(ams_contexts,  dim=0)\n","  ams_contents  =  torch.cat(ams_contents,  dim=0)\n","  labels = torch.cat(labels, dim=0)\n","  labels = labels.to(torch.float64)\n","  ret_obj = [\n","      iids_contexts, \n","      ams_contexts, \n","      iids_contents, \n","      ams_contents,\n","      labels\n","  ]\n","  return ret_obj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wY3j6nyR0py9"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, ConcatDataset\n","from sklearn.model_selection import train_test_split\n","\n","# Note - I think that does it? Gross looking, but straightforward?\n","def create_dataloaders_mc(df_content, question_ids, test_ratio, balance=True):\n","  bigbagohtensors = create_example_tuples_mc(df_content, question_ids, balance=balance)\n","  \n","  iids_ctx = bigbagohtensors[0]\n","  ams_ctx  = bigbagohtensors[1]\n","  iids_cnt = bigbagohtensors[2] \n","  ams_cnt  = bigbagohtensors[3] \n","  labels   = bigbagohtensors[4]  \n","\n","  train_idx, test_idx = train_test_split(\n","      np.arange(len(labels)),\n","      test_size = test_ratio,\n","      shuffle = True)\n","  \n","  # \"Hey bill, should you google about the splat operator? Maybe.\"\n","  train_set = TensorDataset(iids_ctx[train_idx],\n","                            ams_ctx[train_idx],\n","                            iids_cnt[train_idx],\n","                            ams_cnt[train_idx],\n","                            labels[train_idx])\n","\n","  test_set = TensorDataset(iids_ctx[test_idx],\n","                           ams_ctx[test_idx],\n","                           iids_cnt[test_idx],\n","                           ams_cnt[test_idx],\n","                           labels[test_idx])\n","\n","  train_dataloader = DataLoader(train_set,\n","                                batch_size=BATCH_SIZE)\n","\n","  test_dataloader = DataLoader(test_set,\n","                               batch_size=BATCH_SIZE)\n","\n","  return train_dataloader, test_dataloader"]},{"cell_type":"markdown","metadata":{"id":"5SBoofED-TbS"},"source":["### Binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVQsgdTe-uWA"},"outputs":[],"source":["def create_example_tuples_bin(df_content, balance=False):\n","  example_tuples = []\n","  \n","  # Uses the maps created before to obtain token_seqs and attn_masks \n","  # for the questions and contexts. \n","  def make_make_tuple(out_list):\n","    def make_tuple(row):\n","      out_list.append([\n","          map_q_2_ts_am[row['question_id']],\n","          map_ctx_2_ts_am[row['context_id']],\n","          row['content_text'],\n","          row['answer'],\n","          row['context_id'],\n","          row['question_id']\n","      ])\n","    return make_tuple\n","  \n","  df_content.apply(make_make_tuple(example_tuples), axis=1)\n","\n","  if balance:\n","    example_tuples = max_size_oversampling_wr(example_tuples, ctx_idx=4)\n","\n","  labels = [i[3] for i in example_tuples]\n","  iids_questions = []\n","  iids_contexts  = []\n","  iids_contents  = []\n","  ams_questions = []\n","  ams_contexts  = []\n","  ams_contents  = []\n","\n","  for [ts_q, am_q], [ts_ctx, am_ctx], sent, _, _, _ in example_tuples:\n","    iids_questions.append(ts_q)\n","    iids_contexts.append(ts_ctx)\n","    ams_questions.append(am_q)\n","    ams_contexts.append(am_ctx)\n","\n","    enc_dict = tokenizer.encode_plus(\n","        sent,\n","        add_special_tokens = False,\n","        max_length = MAX_SEQ_LEN, \n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","    iids_contents.append(enc_dict['input_ids'])\n","    ams_contents.append(enc_dict['attention_mask'])\n","  \n","  iids_questions = torch.cat(iids_questions, dim=0)\n","  iids_contexts  = torch.cat(iids_contexts,  dim=0)\n","  iids_contents  = torch.cat(iids_contents,  dim=0)\n","  ams_questions =  torch.cat(ams_questions, dim=0)\n","  ams_contexts  =  torch.cat(ams_contexts,  dim=0)\n","  ams_contents  =  torch.cat(ams_contents,  dim=0)\n","  labels = torch.tensor(labels)\n","  labels = torch.reshape(labels, (-1,1))\n","  labels = labels.to(torch.float64)\n","  ret_obj = [\n","      iids_questions, \n","      ams_questions, \n","      iids_contexts, \n","      ams_contexts, \n","      iids_contents, \n","      ams_contents,\n","      labels\n","  ]\n","  return ret_obj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBs0G70x-uWM"},"outputs":[],"source":["def create_dataloaders_bin(df_content, test_ratio, balance=False):\n","  bigbagohtensors = create_example_tuples_bin(df_content, balance=balance)\n","  \n","  iids_q   = bigbagohtensors[0]\n","  ams_q    = bigbagohtensors[1] \n","  iids_ctx = bigbagohtensors[2]\n","  ams_ctx  = bigbagohtensors[3]\n","  iids_cnt = bigbagohtensors[4] \n","  ams_cnt  = bigbagohtensors[5] \n","  labels   = bigbagohtensors[6]  \n","\n","  train_idx, test_idx = train_test_split(\n","      np.arange(len(labels)),\n","      test_size = test_ratio,\n","      shuffle = True,\n","      stratify = labels)\n","  \n","  # \"Hey bill, should you google about the splat operator? Maybe.\"\n","  train_set = TensorDataset(iids_q[train_idx],\n","                            ams_q[train_idx],\n","                            iids_ctx[train_idx],\n","                            ams_ctx[train_idx],\n","                            iids_cnt[train_idx],\n","                            ams_cnt[train_idx],\n","                            labels[train_idx])\n","\n","  test_set = TensorDataset(iids_q[test_idx],\n","                           ams_q[test_idx],\n","                           iids_ctx[test_idx],\n","                           ams_ctx[test_idx],\n","                           iids_cnt[test_idx],\n","                           ams_cnt[test_idx],\n","                           labels[test_idx])\n","\n","  train_dataloader = DataLoader(train_set,\n","                                batch_size=BATCH_SIZE)\n","\n","  test_dataloader = DataLoader(test_set,\n","                               batch_size=BATCH_SIZE)\n","  \n","  return train_dataloader, test_dataloader, train_set, test_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhU3385Ls9c0"},"outputs":[],"source":["def filter_with_model(data_loader, model):\n","  agree_q_ts   = []\n","  agree_q_am   = []\n","  agree_ctx_ts = []\n","  agree_cts_am = []\n","  agree_cnt_ts = []\n","  agree_cnt_am = []\n","  agree_labels = []\n","\n","  model.eval()\n","  for batch in data_loader:\n","    batch = tuple(t.to(cuda_dev) for t in batch)\n","    _, _, _, _, _, _, labels = batch\n","\n","    out = model(batch)\n","    y_pred = torch.round(torch.sigmoid(out))\n","    agree_idxs = (y_pred == labels).nonzero()\n","\n","    for id in agree_idxs:\n","      # print(id)\n","      agree_q_ts.append(batch[0][id, :])\n","      agree_q_am.append(batch[1][id, :])   \n","      agree_ctx_ts.append(batch[2][id, :])\n","      agree_cts_am.append(batch[3][id, :]) \n","      agree_cnt_ts.append(batch[4][id, :]) \n","      agree_cnt_am.append(batch[5][id, :]) \n","      agree_labels.append(batch[6][id]) \n","  \n","  agree_labels = torch.reshape(torch.cat(agree_labels, dim=0), (-1,1)).to(torch.float64).cpu()\n","  dataset = TensorDataset(\n","        torch.cat(agree_q_ts, dim=0).cpu(),\n","        torch.cat(agree_q_am, dim=0).cpu(), \n","        torch.cat(agree_ctx_ts, dim=0).cpu(),\n","        torch.cat(agree_cts_am, dim=0).cpu(),\n","        torch.cat(agree_cnt_ts, dim=0).cpu(),\n","        torch.cat(agree_cnt_am, dim=0).cpu(),\n","        agree_labels\n","  )\n","  return dataset"]},{"cell_type":"markdown","metadata":{"id":"09LneML8n9L3"},"source":["# AQA CCQ Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYqqeGe8zVd8"},"outputs":[],"source":["import torch.nn.functional as F\n","from torch.nn import Module, RNN, Linear, BCELoss, LogSoftmax, BatchNorm1d\n","\n","NUM_BERT_FEATURES = 768\n","NUM_QUESTIONS = 7\n","\n","HL_QUESTION = 128\n","HL_CONTEXT = 128\n","HL_CONTENT = 128\n","HL_PRED = 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NC3L1hdroGuY"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQAM_B_CCQ(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        # 3 MLP Stacks\n","        self.q_linear1   = Linear(NUM_BERT_FEATURES, HL_QUESTION)\n","        self.ctx_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTEXT)\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers\n","        self.pred_linear1 = Linear(HL_QUESTION+HL_CONTEXT+HL_CONTENT, HL_PRED)\n","        self.pred_linear2 = Linear(HL_PRED, 1)\n","\n","        # Batch Norms\n","        self.bn_q    = BatchNorm1d(HL_QUESTION)\n","        self.bn_ctx  = BatchNorm1d(HL_CONTEXT)\n","        self.bn_cnt  = BatchNorm1d(HL_CONTENT) \n","\n","        # self.bn_triple = BatchNorm1d(HL_QUESTION+HL_CONTEXTS+HL_CONTENT)\n","        self.bn_cat  = BatchNorm1d(HL_PRED) \n","\n","    def forward(self, data):\n","        tok_seq_q,   attn_mask_q   = data[0], data[1]\n","        tok_seq_ctx, attn_mask_ctx = data[2], data[3]\n","        tok_seq_cnt, attn_mask_cnt = data[4], data[5]\n","        \n","        emb_q   = self.bert_layer(tok_seq_q,   \n","                                  attention_mask=attn_mask_q)[0][:, 0, :]\n","        emb_ctx = self.bert_layer(tok_seq_ctx, \n","                                  attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","\n","        # The 3 Heads - Question, Context, Content spaces.\n","        ls_q   = self.q_linear1(emb_q)\n","        # ls_q   = self.bn_q(ls_q)\n","        ls_q   = F.relu(ls_q)\n","\n","        ls_ctx = self.ctx_linear1(emb_ctx)\n","        # ls_ctx = self.bn_ctx(ls_ctx)\n","        ls_ctx = F.relu(ls_ctx)\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        # ls_cnt = self.bn_cnt(ls_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","\n","        ls_cat = torch.cat((ls_q, ls_ctx, ls_cnt), dim=1)\n","        \n","        # Prediction Layers\n","        ls_cat = self.pred_linear1(ls_cat)\n","        # ls_cat = self.bn_cat(ls_cat)\n","        ls_cat = F.relu(ls_cat)\n","        return self.pred_linear2(ls_cat)"]},{"cell_type":"markdown","metadata":{"id":"h6ilu9ax8bze"},"source":["# Training and Evaluation Method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gA1bnyGu5nH"},"outputs":[],"source":["def binary_acc(y_pred, y_test):\n","  y_pred_tag = torch.round(torch.sigmoid(y_pred))\n","  correct_results_sum = (y_pred_tag == y_test).sum().float()\n","  acc = correct_results_sum/float(y_test.shape[0])\n","  return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynT43r7n9s2P"},"outputs":[],"source":["def train_and_evaluate(model, metric, training_batches, test_set_batches, epochs, cuda_dev):\n","  optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n","  criterion = torch.nn.BCEWithLogitsLoss().to(cuda_dev)\n","\n","  train_accs = []\n","  test_accs  = [[] for _ in test_set_batches]\n","  for epoch in tqdm(range(epochs), unit=\"epoch\"):\n","    model.train()\n","    \n","    batch_accs = []\n","    for batch in training_batches:\n","      batch = tuple(t.to(cuda_dev) for t in batch)\n","      labels = batch[-1]\n","\n","      optimizer.zero_grad()\n","      out = model(batch)\n","      loss = criterion(out, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      batch_accs.append(metric(out, labels))\n","\n","    train_accs.append(sum(batch_accs)/float(len(batch_accs)))\n","\n","    model.eval()\n","    for t_idx, test_batches in enumerate(test_set_batches):\n","      test_batch_accs = []\n","      for test_batch in test_batches:\n","        test_batch = tuple(t.to(cuda_dev) for t in test_batch)\n","        test_labels = test_batch[-1]\n","\n","        test_out = model(test_batch)\n","        test_batch_accs.append(metric(test_out, test_labels))\n","      \n","      test_accs[t_idx].append(sum(test_batch_accs)/float(len(test_batch_accs)))\n","\n","  return train_accs, test_accs, model"]},{"cell_type":"markdown","metadata":{"id":"Rc-9BtlyoVO-"},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{"id":"lFedpuoVoY98"},"source":["..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asj8yT0FxeUy"},"outputs":[],"source":["RESULTS_NOTE = \"00_first_full_run\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZY4Qz_jXobwN"},"outputs":[],"source":["cuda_dev  = torch.device(\"cuda:0\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZNjrMOHom9c"},"outputs":[],"source":["A3_mp_train_dl, A3_mp_test_dl, A3_mp_train_ds, A3_mp_test_ds = create_dataloaders_bin(content_a3_majplus, \n","                                             0.2,\n","                                             balance=True)\n","\n","A2_fa_train_dl, A2_fa_test_dl, A2_fa_train_ds, A2_fa_test_ds = create_dataloaders_bin(content_a2_fa, \n","                                             0.2, \n","                                             balance=True)\n","\n","A23_mp_train_dl = DataLoader(ConcatDataset([A3_mp_train_ds, A2_fa_train_ds]))\n","\n","test_batches = [\n","  A3_mp_test_dl,\n","  A2_fa_test_dl\n","]"]},{"cell_type":"markdown","metadata":{"id":"krVuGQZnpIDB"},"source":["## Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["8f8f6094955b43a88f6594b7e4c82aa5","f7d4361539854403805311369e7e1213","26796e88ef4f44268a79f05669b95907","ee285a1416c14d4683ecdc0ca4c30cd6","55d234331ce847ea9dea0fe36600f7b0","e5d05f6487b64266b0d3a8e0362a6699","e41e053058de4bc69796092b12640680","4e9a08d6d916433bae027d767722fea3","94c933952ba640a6ad0398836d084450","d32dfa8f94f44412bcf10cab586f6509","f3fd665462db4ef4aa394e5931a8e186"]},"id":"qlci2w5OpKHF","outputId":"950f02a9-2ade-4749-fd00-4ce11e541e16"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f8f6094955b43a88f6594b7e4c82aa5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_baseline = AQAM_B_CCQ().to(cuda_dev)\n","\n","train_baseline_pre, test_baseline_pre, model_baseline = train_and_evaluate(\n","    model_baseline,\n","    binary_acc,\n","    A3_mp_train_dl, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["8304f64bb160434b80e59140576e8a0f"]},"id":"MYzhNS4pqAF6","outputId":"623cd865-1904-4e0b-b521-1b33c336c88d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8304f64bb160434b80e59140576e8a0f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_baseline_post, test_baseline_post, model_baseline = train_and_evaluate(\n","    model_baseline,\n","    binary_acc,\n","    A3_mp_train_dl, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"markdown","metadata":{"id":"vetQbRDGpKbX"},"source":["## Full Aug"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["2ab7b429b0534ed199966b72f84ce83d"]},"id":"FRotkguzpL3J","outputId":"e20a1c09-e203-4f43-8f02-61b984b5e350"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ab7b429b0534ed199966b72f84ce83d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_full_aug = AQAM_B_CCQ().to(cuda_dev)\n","\n","train_full_aug_pre, test_full_aug_pre, model_full_aug = train_and_evaluate(\n","    model_full_aug,\n","    binary_acc,\n","    A3_mp_train_dl, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["e6c4fbd7a90f4ca7a6360e22c8f0640a"]},"id":"q2E1bdjLqRyf","outputId":"10dd89d7-0b0a-48e1-9879-ae7feb941971"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6c4fbd7a90f4ca7a6360e22c8f0640a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Note - now we pass the combined dataset - A23_mp_train_dl\n","train_full_aug_post, test_full_aug_post, model_full_aug = train_and_evaluate(\n","    model_full_aug,\n","    binary_acc,\n","    A23_mp_train_dl, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"markdown","metadata":{"id":"_sEkmuUypMya"},"source":["## Filtered Aug"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["980be3db88464901b1147be618449b7f"]},"id":"E6dHRgfNpOoV","outputId":"fdbe5cfb-5541-4849-d3eb-518570a49afb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"980be3db88464901b1147be618449b7f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_filter_aug = AQAM_B_CCQ().to(cuda_dev)\n","\n","train_filter_aug_pre, test_filter_aug_pre, model_filter_aug = train_and_evaluate(\n","    model_filter_aug,\n","    binary_acc,\n","    A3_mp_train_dl, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TF3I_ZJ0sgF9"},"outputs":[],"source":["# Filter the non-agree examples out of the dataset. \n","A2_fa_filtered_ds = filter_with_model(A2_fa_train_dl, model_filter_aug)\n","A23_filter_aug_dl = DataLoader(ConcatDataset([A3_mp_train_ds, A2_fa_filtered_ds]))"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34,"referenced_widgets":["4ed0fa9a321e4a3ba790e12fd5645c14"]},"id":"FJ6rGC1JsgWt","executionInfo":{"status":"ok","timestamp":1679068314484,"user_tz":240,"elapsed":9199639,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}},"outputId":"ffa6bd3b-6a61-4c9e-c1b0-a1d6485d985f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ed0fa9a321e4a3ba790e12fd5645c14","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_filter_aug_post, test_filter_aug_post, model_filter_aug = train_and_evaluate(\n","    model_filter_aug,\n","    binary_acc,\n","    A23_filter_aug_dl, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"markdown","metadata":{"id":"R6qNfDz4xamf"},"source":["# Results"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"QkQinqFhxcL0","executionInfo":{"status":"ok","timestamp":1679068317130,"user_tz":240,"elapsed":2647,"user":{"displayName":"Bill Power","userId":"17299628813846429758"}}},"outputs":[],"source":["results_object = [\n","  [train_baseline_pre, test_baseline_pre, train_baseline_post, test_baseline_post],\n","  [train_full_aug_pre, test_full_aug_pre, train_full_aug_post, test_full_aug_post],\n","  [train_filter_aug_pre, test_filter_aug_pre, train_filter_aug_post, test_filter_aug_post]\n","]\n","\n","results_obj_fn = f\"{RESULTS_NOTE}_raw_results_obj.p\"\n","with open(results_obj_fn, 'wb') as f:\n","  pickle.dump(results_object, f)\n","\n","upload_file_to_gcs(results_obj_fn, f\"{RESULTS_GCS_DIR}/{results_obj_fn}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["FC4VN9OX7ASA","ybu8iEZskb-3","TBwBOc1LJWSS","YEniTxLUPL-K","kNQBRgQkkb_B","noOpZRP8yXpr","Ee2A1EDh0Zgd","09LneML8n9L3","h6ilu9ax8bze"],"machine_shape":"hm","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyMeQT/yHY/fku85yfRk438G"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"26796e88ef4f44268a79f05669b95907":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e9a08d6d916433bae027d767722fea3","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94c933952ba640a6ad0398836d084450","value":8}},"4e9a08d6d916433bae027d767722fea3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55d234331ce847ea9dea0fe36600f7b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f8f6094955b43a88f6594b7e4c82aa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7d4361539854403805311369e7e1213","IPY_MODEL_26796e88ef4f44268a79f05669b95907","IPY_MODEL_ee285a1416c14d4683ecdc0ca4c30cd6"],"layout":"IPY_MODEL_55d234331ce847ea9dea0fe36600f7b0"}},"94c933952ba640a6ad0398836d084450":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d32dfa8f94f44412bcf10cab586f6509":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e41e053058de4bc69796092b12640680":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5d05f6487b64266b0d3a8e0362a6699":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee285a1416c14d4683ecdc0ca4c30cd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d32dfa8f94f44412bcf10cab586f6509","placeholder":"​","style":"IPY_MODEL_f3fd665462db4ef4aa394e5931a8e186","value":" 8/25 [15:11&lt;32:16, 113.90s/epoch]"}},"f3fd665462db4ef4aa394e5931a8e186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7d4361539854403805311369e7e1213":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5d05f6487b64266b0d3a8e0362a6699","placeholder":"​","style":"IPY_MODEL_e41e053058de4bc69796092b12640680","value":" 32%"}}}}},"nbformat":4,"nbformat_minor":0}