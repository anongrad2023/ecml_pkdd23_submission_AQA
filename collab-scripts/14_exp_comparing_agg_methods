{"cells":[{"cell_type":"markdown","metadata":{"id":"E2UNaoi3uMlP"},"source":["# Overview\n"]},{"cell_type":"markdown","metadata":{"id":"RNJC4tXOuONp"},"source":["TODO - Write up what this is.\n","\n","Looking at other agg's besides concatenating. "]},{"cell_type":"markdown","metadata":{"id":"AbeAB8Hokb-2"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"0fSMAjHpkb-4"},"source":["## Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNU4HLNZkb-5"},"outputs":[],"source":["RESULTS_GCS_DIR = \"results/AQA14\"\n","\n","GCS_APP_ID = \"aqa-research\"\n","GCS_BUCKET = \"dabi-aqa-data-00\"\n","\n","FN_QUESTIONS = \"questions_01.csv\"\n","FN_CONTEXTS  = \"contexts_01.csv\"\n","\n","FN_A2_RAW = \"a2_raw.csv\"\n","FN_A2_FA  = \"a2_gs.csv\"\n","FN_A3_FA  = \"a3_gs.csv\"\n","FN_A3_MAJ = \"a3_con.csv\"\n","\n","MAX_SEQ_LEN = 128\n","\n","EPOCHS = 50\n","BATCH_SIZE = 16\n","TEST_FRAC = 0.1\n","\n","RAND_SEED = 4246"]},{"cell_type":"markdown","metadata":{"id":"FC4VN9OX7ASA"},"source":["## Pippin'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6292,"status":"ok","timestamp":1679324609054,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"MjPN6S5z6_7_","outputId":"32deb10f-aaa0-4b89-a7ac-cbe215a5ca79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch\u003e=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n","Requirement already satisfied: numpy\u003e=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch\u003e=1.8.1-\u003etorchmetrics) (4.5.0)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.4\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10643,"status":"ok","timestamp":1679324619676,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"F0uhaIcV7LtL","outputId":"1aa9b24a-1de1-46d1-e0f9-449404463972"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (4.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2022.12.7)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"ybu8iEZskb-3"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiAEt7P6kb-3"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import torch\n","import random\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from tqdm.notebook import tqdm\n","import pickle\n","\n","import transformers as ppb\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","\n","from torch.optim import SGD\n","from torchmetrics.classification import MultilabelAccuracy"]},{"cell_type":"markdown","metadata":{"id":"glKP3nrhkb-6"},"source":["## GCS Auth - Input Needed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26874,"status":"ok","timestamp":1679324651163,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"xwxl9-b4kb-7","outputId":"5cf34ed8-8445-4b72-faff-9da6f1c61d47"},"outputs":[{"name":"stdout","output_type":"stream","text":["Go to the following link in your browser:\n","\n","    https://accounts.google.com/o/oauth2/auth?response_type=code\u0026client_id=32555940559.apps.googleusercontent.com\u0026redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html\u0026scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\u0026state=ZmaIytCd9U5CAScIirOMzsVNU4W7XD\u0026prompt=consent\u0026access_type=offline\u0026code_challenge=Xv5eohZw8U9kc-QbY8wdBXm7-n03gfbbpzGjsvhKjj0\u0026code_challenge_method=S256\n","\n","Enter authorization code: 4/0AWtgzh6hZVErDjbCT3rw4efMu4P8M9VVIxcVdX9wSWVg9SXAQK0YxcbZxekbsrsTJM53cQ\n","\n","You are now logged in as [willpowe@gmail.com].\n","Your current project is [None].  You can change this setting by running:\n","  $ gcloud config set project PROJECT_ID\n"]}],"source":["!gcloud auth login --launch-browser"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1521,"status":"ok","timestamp":1679324652663,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"SEQdfjlikb-7","outputId":"a6c6a8cb-8745-416d-9014-cd3f8ac72c98"},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated property [core/project].\n"]}],"source":["!gcloud config set project {GCS_APP_ID}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x_hDs2gkb-8"},"outputs":[],"source":["def download_file_from_gcs(src_fn, dest_fn):\n","  dest = f\"/content/{dest_fn}\"\n","  dl_command = f\"gsutil -m cp gs://{GCS_BUCKET}/{src_fn} {dest}\"\n","  os.system(dl_command)\n","\n","def upload_file_to_gcs(src_fn, dest_fn):\n","  dest_url = \"{}/{}\".format(GCS_BUCKET, dest_fn)\n","  ul_command = \"gsutil -m cp {} gs://{}\".format(src_fn, dest_url)\n","  os.system(ul_command)"]},{"cell_type":"markdown","metadata":{"id":"TBwBOc1LJWSS"},"source":["## Random Seeds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1679324652666,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"HNrkkYVqJV62","outputId":"e1db09bb-d510-4e24-f146-dcbba7c69e3d"},"outputs":[{"data":{"text/plain":["\u003ctorch._C.Generator at 0x7fcf0f2adab0\u003e"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["random.seed(RAND_SEED)\n","np.random.seed(RAND_SEED)\n","torch.manual_seed(RAND_SEED)"]},{"cell_type":"markdown","metadata":{"id":"YEniTxLUPL-K"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"Df9iIZyhkb--"},"source":["## Raw Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOIG4PnXkb-_"},"outputs":[],"source":["download_file_from_gcs(FN_QUESTIONS, FN_QUESTIONS)\n","download_file_from_gcs(FN_CONTEXTS, FN_CONTEXTS)\n","\n","download_file_from_gcs(FN_A2_RAW, FN_A2_RAW)\n","download_file_from_gcs(FN_A2_FA, FN_A2_FA)\n","download_file_from_gcs(FN_A3_FA, FN_A3_FA)\n","download_file_from_gcs(FN_A3_MAJ, FN_A3_MAJ)"]},{"cell_type":"markdown","metadata":{"id":"CnejujZg8M8q"},"source":["## Dataframes"]},{"cell_type":"markdown","metadata":{"id":"pxorHwUB8Ro-"},"source":["### Question/Context Dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PryV0dbr8SOA"},"outputs":[],"source":["questions = pd.read_csv(FN_QUESTIONS)\n","contexts  = pd.read_csv(FN_CONTEXTS)"]},{"cell_type":"markdown","metadata":{"id":"zUUz0lb2yLAg"},"source":["### Binary View Dataframes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuLRs3tXkb_C"},"outputs":[],"source":["content_a2_raw = pd.read_csv(FN_A2_RAW)\n","content_a2_fa  = pd.read_csv(FN_A2_FA)\n","content_a3_fa  = pd.read_csv(FN_A3_FA)\n","content_a3_maj = pd.read_csv(FN_A3_MAJ)\n","\n","# Could also try this with using the 3's as 0's\n","content_a2_raw = content_a2_raw[content_a2_raw['answer'] != 3] \n","content_a2_fa  = content_a2_fa[content_a2_fa['answer'] != 3] \n","content_a3_fa  = content_a3_fa[content_a3_fa['answer'] != 3] \n","content_a3_maj = content_a3_maj[content_a3_maj['answer'] != 3] \n","\n","content_a3_majplus = pd.concat([content_a3_fa, content_a3_maj])\n","content_a3a2_mp    = pd.concat([content_a3_fa, content_a3_maj, content_a2_fa])"]},{"cell_type":"markdown","metadata":{"id":"kNQBRgQkkb_B"},"source":["## LLM Initalization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"elapsed":7609,"status":"ok","timestamp":1679324676343,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"wrotnPAHkb_C","outputId":"6aa8bb7c-e9dd-44ee-84b9-c4ef14eb10be"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67a6beb74d574e16ab4bb9f6b8d470b1","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82097357ecc34107a2dbff929974151e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"314087d31cd24772850dca8a585eda99","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# For DistilBERT:\n","model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n","\n","## Want BERT instead of distilBERT? Uncomment the following line:\n","#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","\n","# TODO - figure out the above for the Q/A model. Might need a different\n","#        enough implementation/approach that it warrents a dif script. \n","#        It takes a masked input for the Q and options for multiple choice? idk. \n","\n","# Load pretrained model/tokenizer\n","tokenizer  = tokenizer_class.from_pretrained(pretrained_weights, truncate=True,)"]},{"cell_type":"markdown","metadata":{"id":"noOpZRP8yXpr"},"source":["## Question/Context Embedding Dicts"]},{"cell_type":"markdown","metadata":{"id":"IOoeD1gRUkyE"},"source":["Used later to build the individual training examples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad4YwoEX2otO"},"outputs":[],"source":["def build_ts_am_map(df_in, text_col_name, id_col):\n","  ids   = df_in[id_col].values\n","  sents = df_in[text_col_name].values\n","\n","  token_seqs = []\n","  attn_masks = []\n","  id_2_tsam = dict()\n","  for id, sent in zip(ids, sents):\n","    enc_dict = tokenizer.encode_plus(\n","      sent,\n","      add_special_tokens = False,\n","      max_length = MAX_SEQ_LEN, \n","      pad_to_max_length = True,\n","      return_attention_mask = True,\n","      return_tensors = 'pt'\n","    )\n","    id_2_tsam[id] = [enc_dict['input_ids'], enc_dict['attention_mask']]\n","\n","  return id_2_tsam"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1679324676345,"user":{"displayName":"Bill Power","userId":"17299628813846429758"},"user_tz":240},"id":"35ukWIxGuq_t","outputId":"f0b50bc7-2c65-4cdb-e44f-776c36619cdc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]}],"source":["map_q_2_ts_am   = build_ts_am_map(questions, 'question_text', 'question_id')\n","map_ctx_2_ts_am = build_ts_am_map(contexts, 'context_text', 'context_id')"]},{"cell_type":"markdown","metadata":{"id":"Ee2A1EDh0Zgd"},"source":["## Balancing - Over-Sampling Method\n","\n","Tweaked for MC."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8b-jBO3zaM9"},"outputs":[],"source":["# The ctx_idx will change depending on if this is being passed Binary or MC data\n","# 2 =\u003e MC\n","# ? =\u003e Binary\n","def max_size_oversampling_wr(examples, ctx_idx=2):\n","  question_counts = dict()\n","  context_counts  = dict()\n","  \n","  for ex in examples:\n","    ctx = ex[ctx_idx]\n","    if ctx not in context_counts:\n","      context_counts[ctx] = 0\n","    context_counts[ctx] += 1\n","\n","  context_examples    = { k: [] for k in context_counts}\n","  context_populations = { k: [] for k in context_counts}\n","\n","  for example in examples:\n","    ctx = example[ctx_idx] # This might change depending on shape of examples.\n","    context_populations[ctx].append(example)\n","\n","  max_size = 0\n","  for k in context_populations:\n","    size_pop = len(context_populations[k])\n","    if size_pop \u003e max_size:\n","      max_size = size_pop\n","\n","  for k in context_examples:\n","    while len(context_examples[k]) \u003c max_size:\n","      context_examples[k].append(random.sample(context_populations[k], 1)[0])\n","\n","  example_tuples_max_sampled = []\n","  for k in context_examples:\n","    for ex in context_examples[k]:\n","      example_tuples_max_sampled.append(ex)\n","  return example_tuples_max_sampled"]},{"cell_type":"markdown","metadata":{"id":"7EFQMMop0fyz"},"source":["## DataLoaders\n"]},{"cell_type":"markdown","metadata":{"id":"5SBoofED-TbS"},"source":["### Binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVQsgdTe-uWA"},"outputs":[],"source":["def create_example_tuples_bin(df_content, balance=False):\n","  example_tuples = []\n","  \n","  # Uses the maps created before to obtain token_seqs and attn_masks \n","  # for the questions and contexts. \n","  def make_make_tuple(out_list):\n","    def make_tuple(row):\n","      out_list.append([\n","          map_q_2_ts_am[row['question_id']],\n","          map_ctx_2_ts_am[row['context_id']],\n","          row['content_text'],\n","          row['answer'],\n","          row['context_id'],\n","          row['question_id']\n","      ])\n","    return make_tuple\n","  \n","  df_content.apply(make_make_tuple(example_tuples), axis=1)\n","\n","  if balance:\n","    example_tuples = max_size_oversampling_wr(example_tuples, ctx_idx=4)\n","\n","  labels = [i[3] for i in example_tuples]\n","  iids_questions = []\n","  iids_contexts  = []\n","  iids_contents  = []\n","  ams_questions = []\n","  ams_contexts  = []\n","  ams_contents  = []\n","\n","  for [ts_q, am_q], [ts_ctx, am_ctx], sent, _, _, _ in example_tuples:\n","    iids_questions.append(ts_q)\n","    iids_contexts.append(ts_ctx)\n","    ams_questions.append(am_q)\n","    ams_contexts.append(am_ctx)\n","\n","    enc_dict = tokenizer.encode_plus(\n","        sent,\n","        add_special_tokens = False,\n","        max_length = MAX_SEQ_LEN, \n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","    )\n","    iids_contents.append(enc_dict['input_ids'])\n","    ams_contents.append(enc_dict['attention_mask'])\n","  \n","  iids_questions = torch.cat(iids_questions, dim=0)\n","  iids_contexts  = torch.cat(iids_contexts,  dim=0)\n","  iids_contents  = torch.cat(iids_contents,  dim=0)\n","  ams_questions =  torch.cat(ams_questions, dim=0)\n","  ams_contexts  =  torch.cat(ams_contexts,  dim=0)\n","  ams_contents  =  torch.cat(ams_contents,  dim=0)\n","  labels = torch.tensor(labels)\n","  labels = torch.reshape(labels, (-1,1))\n","  labels = labels.to(torch.float64)\n","  ret_obj = [\n","      iids_questions, \n","      ams_questions, \n","      iids_contexts, \n","      ams_contexts, \n","      iids_contents, \n","      ams_contents,\n","      labels\n","  ]\n","  return ret_obj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBs0G70x-uWM"},"outputs":[],"source":["def create_dataloaders_bin(df_content, test_ratio, balance=False):\n","  bigbagohtensors = create_example_tuples_bin(df_content, balance=balance)\n","  \n","  iids_q   = bigbagohtensors[0]\n","  ams_q    = bigbagohtensors[1] \n","  iids_ctx = bigbagohtensors[2]\n","  ams_ctx  = bigbagohtensors[3]\n","  iids_cnt = bigbagohtensors[4] \n","  ams_cnt  = bigbagohtensors[5] \n","  labels   = bigbagohtensors[6]  \n","\n","  train_idx, test_idx = train_test_split(\n","      np.arange(len(labels)),\n","      test_size = test_ratio,\n","      shuffle = True,\n","      stratify = labels)\n","  \n","  # \"Hey bill, should you google about the splat operator? Maybe.\"\n","  train_set = TensorDataset(iids_q[train_idx],\n","                            ams_q[train_idx],\n","                            iids_ctx[train_idx],\n","                            ams_ctx[train_idx],\n","                            iids_cnt[train_idx],\n","                            ams_cnt[train_idx],\n","                            labels[train_idx])\n","\n","  test_set = TensorDataset(iids_q[test_idx],\n","                           ams_q[test_idx],\n","                           iids_ctx[test_idx],\n","                           ams_ctx[test_idx],\n","                           iids_cnt[test_idx],\n","                           ams_cnt[test_idx],\n","                           labels[test_idx])\n","\n","  train_dataloader = DataLoader(train_set,\n","                                batch_size=BATCH_SIZE)\n","\n","  test_dataloader = DataLoader(test_set,\n","                               batch_size=BATCH_SIZE)\n","  \n","  return train_dataloader, test_dataloader"]},{"cell_type":"markdown","metadata":{"id":"rPCF7MZNzOAT"},"source":["# Model Implementations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYqqeGe8zVd8"},"outputs":[],"source":["import torch.nn.functional as F\n","from torch.nn import Module, RNN, Linear, BCELoss, LogSoftmax, BatchNorm1d\n","\n","NUM_BERT_FEATURES = 768\n","NUM_QUESTIONS = 7\n","\n","HL_QUESTION = 128\n","HL_CONTEXT  = 128\n","HL_CONTENT  = 128\n","HL_PRED = 8"]},{"cell_type":"markdown","metadata":{"id":"TbrkrVp9unHC"},"source":["## AQA-CCQ-CAT\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDvRHzV2unHF"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQACCQ_CAT(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        # 3 MLP Stacks\n","        self.q_linear1   = Linear(NUM_BERT_FEATURES, HL_QUESTION)\n","        self.ctx_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTEXT)\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers\n","        self.pred_linear1 = Linear(HL_QUESTION+HL_CONTEXT+HL_CONTENT, HL_PRED)\n","        self.pred_linear2 = Linear(HL_PRED, 1)\n","\n","        # Batch Norms\n","        self.bn_q    = BatchNorm1d(HL_QUESTION)\n","        self.bn_ctx  = BatchNorm1d(HL_CONTEXT)\n","        self.bn_cnt  = BatchNorm1d(HL_CONTENT) \n","\n","        # self.bn_triple = BatchNorm1d(HL_QUESTION+HL_CONTEXTS+HL_CONTENT)\n","        self.bn_cat  = BatchNorm1d(HL_PRED) \n","\n","    def forward(self, data):\n","        tok_seq_q,   attn_mask_q   = data[0], data[1]\n","        tok_seq_ctx, attn_mask_ctx = data[2], data[3]\n","        tok_seq_cnt, attn_mask_cnt = data[4], data[5]\n","        \n","        emb_q   = self.bert_layer(tok_seq_q,   \n","                                  attention_mask=attn_mask_q)[0][:, 0, :]\n","        emb_ctx = self.bert_layer(tok_seq_ctx, \n","                                  attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","\n","        # The 3 Heads - Question, Context, Content spaces.\n","        ls_q   = self.q_linear1(emb_q)\n","        # ls_q   = self.bn_q(ls_q)\n","        ls_q   = F.relu(ls_q)\n","\n","        ls_ctx = self.ctx_linear1(emb_ctx)\n","        # ls_ctx = self.bn_ctx(ls_ctx)\n","        ls_ctx = F.relu(ls_ctx)\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        # ls_cnt = self.bn_cnt(ls_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","\n","        ls_cat = torch.cat((ls_q, ls_ctx, ls_cnt), dim=1)\n","        \n","        # Prediction Layers\n","        ls_cat = self.pred_linear1(ls_cat)\n","        # ls_cat = self.bn_cat(ls_cat)\n","        ls_cat = F.relu(ls_cat)\n","        return self.pred_linear2(ls_cat)"]},{"cell_type":"markdown","metadata":{"id":"tYJ9syWYd_fl"},"source":["## AQA-CCQ-SUM\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26wK50Rad_fn"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQACCQ_SUM(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        # 3 MLP Stacks\n","        self.q_linear1   = Linear(NUM_BERT_FEATURES, HL_QUESTION)\n","        self.ctx_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTEXT)\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers -\u003e Assumes HL's are all the same size\n","        self.pred_linear1 = Linear(HL_QUESTION, HL_PRED)\n","        self.pred_linear2 = Linear(HL_PRED, 1)\n","\n","    def forward(self, data):\n","        tok_seq_q,   attn_mask_q   = data[0], data[1]\n","        tok_seq_ctx, attn_mask_ctx = data[2], data[3]\n","        tok_seq_cnt, attn_mask_cnt = data[4], data[5]\n","        \n","        emb_q   = self.bert_layer(tok_seq_q,   \n","                                  attention_mask=attn_mask_q)[0][:, 0, :]\n","        emb_ctx = self.bert_layer(tok_seq_ctx, \n","                                  attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","\n","        # The 3 Heads - Question, Context, Content spaces.\n","        ls_q   = self.q_linear1(emb_q)\n","        ls_q   = F.relu(ls_q)\n","\n","        ls_ctx = self.ctx_linear1(emb_ctx)\n","        ls_ctx = F.relu(ls_ctx)\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","\n","        # We have 3 BATCHxEMB_DIM, we want BATCHxEMB out. I think this gets us that?\n","        ls_sum = torch.sum(torch.stack([ls_q, ls_ctx, ls_cnt], dim=2), dim=2)\n","        \n","        # Prediction Layers\n","        ls_sum = self.pred_linear1(ls_sum)\n","        ls_sum = F.relu(ls_sum)\n","        return self.pred_linear2(ls_sum)"]},{"cell_type":"markdown","metadata":{"id":"yLQzBvCpeB2g"},"source":["## AQA-CCQ-AVE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_kkTTl7eB2h"},"outputs":[],"source":["# TODO - Update model arch.\n","class AQACCQ_AVE(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Transformer Embedding Layer\n","        self.bert_layer = model_class.from_pretrained(pretrained_weights,    \n","                                                      num_labels = 2,\n","                                                      output_attentions = False,\n","                                                      output_hidden_states = False,\n","                                                      return_dict=False)\n","\n","        # 3 MLP Stacks\n","        self.q_linear1   = Linear(NUM_BERT_FEATURES, HL_QUESTION)\n","        self.ctx_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTEXT)\n","        self.content_linear1 = Linear(NUM_BERT_FEATURES, HL_CONTENT)\n","        \n","        # Prediction Layers -\u003e Assumes HL's are all the same size\n","        self.pred_linear1 = Linear(HL_QUESTION, HL_PRED)\n","        self.pred_linear2 = Linear(HL_PRED, 1)\n","\n","    def forward(self, data):\n","        tok_seq_q,   attn_mask_q   = data[0], data[1]\n","        tok_seq_ctx, attn_mask_ctx = data[2], data[3]\n","        tok_seq_cnt, attn_mask_cnt = data[4], data[5]\n","        \n","        emb_q   = self.bert_layer(tok_seq_q,   \n","                                  attention_mask=attn_mask_q)[0][:, 0, :]\n","        emb_ctx = self.bert_layer(tok_seq_ctx, \n","                                  attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_cnt = self.bert_layer(tok_seq_cnt, \n","                                  attention_mask=attn_mask_cnt)[0][:, 0, :]\n","\n","        # The 3 Heads - Question, Context, Content spaces.\n","        ls_q   = self.q_linear1(emb_q)\n","        ls_q   = F.relu(ls_q)\n","\n","        ls_ctx = self.ctx_linear1(emb_ctx)\n","        ls_ctx = F.relu(ls_ctx)\n","        \n","        ls_cnt = self.content_linear1(emb_cnt)\n","        ls_cnt = F.relu(ls_cnt)\n","\n","        # We have 3 BATCHxEMB_DIM, we want BATCHxEMB out. I think this gets us that?\n","        ls_ave = torch.mean(torch.stack([ls_q, ls_ctx, ls_cnt], dim=2), dim=2)\n","        \n","        # Prediction Layers\n","        ls_ave = self.pred_linear1(ls_ave)\n","        ls_ave = F.relu(ls_ave)\n","        return self.pred_linear2(ls_ave)"]},{"cell_type":"markdown","metadata":{"id":"h6ilu9ax8bze"},"source":["# Training and Evaluation Method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynT43r7n9s2P"},"outputs":[],"source":["def train_and_evaluate(model, metric, training_batches, test_set_batches, epochs, cuda_dev):\n","  optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n","  criterion = torch.nn.BCEWithLogitsLoss().to(cuda_dev)\n","\n","  train_accs = []\n","  test_accs  = [[] for _ in test_set_batches]\n","  for epoch in tqdm(range(epochs), unit=\"epoch\"):\n","    model.train()\n","    \n","    batch_accs = []\n","    for batch in training_batches:\n","      batch = tuple(t.to(cuda_dev) for t in batch)\n","      labels = batch[-1]\n","\n","      optimizer.zero_grad()\n","      out = model(batch)\n","      loss = criterion(out, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      batch_accs.append(metric(out, labels))\n","\n","    train_accs.append(sum(batch_accs)/float(len(batch_accs)))\n","\n","    model.eval()\n","    for t_idx, test_batches in enumerate(test_set_batches):\n","      test_batch_accs = []\n","      for test_batch in test_batches:\n","        test_batch = tuple(t.to(cuda_dev) for t in test_batch)\n","        test_labels = test_batch[-1]\n","\n","        test_out = model(test_batch)\n","        test_batch_accs.append(metric(test_out, test_labels))\n","      \n","      test_accs[t_idx].append(sum(test_batch_accs)/float(len(test_batch_accs)))\n","\n","  return train_accs, test_accs, model"]},{"cell_type":"markdown","metadata":{"id":"4F9mZnh3xKZF"},"source":["# Embedding Extraction Methods"]},{"cell_type":"markdown","metadata":{"id":"8Yj20aAZxOPt"},"source":["These take in a model and a dataset, and return the underlying BERT/LLM embeddings for each part of the input tuple for each item/batch in the dataset. Note - This assumes a structure to the model, or atleast that it contains a LLM layer named \"bert_layer\". More complicated architectures (like ones that use a different LLM for context/content/question) will need a different 'gather' method to pull out the embeddings. "]},{"cell_type":"markdown","metadata":{"id":"2kxrcLbaCRmg"},"source":["## Binary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guoAacFHCJCq"},"outputs":[],"source":["def gather_embeddings_bin(model, dataloaders):\n","  model.eval()\n","\n","  embs_cnt     = []\n","  embs_cntctx  = []\n","  embs_cntq    = []\n","  embs_cntctxq = []\n","  agree_flags  = []\n","  for dl in dataloaders:\n","    for batch in dl:\n","      tok_seq_q,   attn_mask_q   = batch[0].to(cuda_dev), batch[1].to(cuda_dev)\n","      tok_seq_ctx, attn_mask_ctx = batch[2].to(cuda_dev), batch[3].to(cuda_dev)\n","      tok_seq_cnt, attn_mask_cnt = batch[4].to(cuda_dev), batch[5].to(cuda_dev)\n","\n","      labels = batch[6].to(cuda_dev)\n","      batch = [b.to(cuda_dev) for b in batch]\n","      with torch.no_grad():\n","        emb_cnt = model.bert_layer(tok_seq_cnt, \n","                                      attention_mask=attn_mask_cnt)[0][:, 0, :]\n","        emb_ctx = model.bert_layer(tok_seq_ctx, \n","                                      attention_mask=attn_mask_ctx)[0][:, 0, :]\n","        emb_q   = model.bert_layer(tok_seq_q,\n","                                      attention_mask=attn_mask_q)[0][:, 0, :]\n","        out = model(batch)\n","        y_pred_tag = torch.round(torch.sigmoid(out))\n","        agree = y_pred_tag == labels\n","\n","      # TODO - Here is where we'll call the full forward pass of the model on\n","      #        the batch, and then generate an agreement vector? flag vector\n","      #        where a_i is 0 if the model disagrees, 1 if it agrees.\n","      # so we get an output, and we want to get the 'labels == output' flags.\n","\n","      embs_cnt.append(emb_cnt.cpu())\n","      embs_cntctx.append(torch.cat([emb_cnt.cpu(), emb_ctx.cpu()], 1))\n","      embs_cntq.append(torch.cat([emb_cnt.cpu(), emb_q.cpu()], 1))\n","      embs_cntctxq.append(torch.cat([emb_cnt.cpu(), emb_ctx.cpu(), emb_q.cpu()], 1))\n","      agree_flags.append(agree.cpu())\n","      # Overkill, but hey, not going OoM on the gpu anymore.\n","      del tok_seq_q\n","      del tok_seq_cnt\n","      del tok_seq_ctx\n","      del attn_mask_q\n","      del attn_mask_cnt\n","      del attn_mask_ctx\n","      del emb_cnt\n","      del emb_ctx\n","      del emb_q\n","      del batch\n","      with torch.no_grad(): # Note to future bill: empty cache only works in a no_grad\n","        torch.cuda.empty_cache()\n","  \n","  return torch.cat(embs_cnt, 0), torch.cat(embs_cntctx, 0), torch.cat(embs_cntq, 0), torch.cat(embs_cntctxq, 0), torch.cat(agree_flags, 0)"]},{"cell_type":"markdown","metadata":{"id":"1UhRSuE9wQHp"},"source":["# Experiments"]},{"cell_type":"markdown","metadata":{"id":"wbxhAQwswYDH"},"source":["Notes.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFmDNG_UheNS"},"outputs":[],"source":["# Prepended to image and data pickles before uploading to GCS. \n","RESULTS_NOTE = \"00_first_run_\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkM6UKd4xkr7"},"outputs":[],"source":["cuda_dev  = torch.device(\"cuda:0\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oW3GllDrDZWw"},"outputs":[],"source":["A23_mp_train, A23_mp_test = create_dataloaders_bin(content_a3a2_mp, \n","                                             0.2,\n","                                             balance=True)\n","\n","A3_fa_train, A3_fa_test = create_dataloaders_bin(content_a3_fa, \n","                                             0.2,\n","                                             balance=False)\n","A2_fa_train, A2_fa_test = create_dataloaders_bin(content_a2_fa, \n","                                             0.2, \n","                                             balance=False)\n","\n","test_batches = [\n","  A23_mp_test,\n","  A3_fa_test,\n","  A2_fa_test\n","]\n","\n","# For generating the 'full set' of tSNE embeddings. \n","A3_a, A3_b = create_dataloaders_bin(content_a3_majplus, \n","                                0.5,\n","                                balance=False)\n","A2_a, A2_b = create_dataloaders_bin(content_a2_fa, \n","                                0.5,\n","                                balance=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_yte4uxRIPh"},"outputs":[],"source":["def binary_acc(y_pred, y_test):\n","  y_pred_tag = torch.round(torch.sigmoid(y_pred))\n","  correct_results_sum = (y_pred_tag == y_test).sum().float()\n","  acc = correct_results_sum/float(y_test.shape[0])\n","  return acc"]},{"cell_type":"markdown","metadata":{"id":"ynJF50QkecOE"},"source":["## AQA-CCQ-CAT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":177},"id":"EkzluySZeedC","outputId":"21e95a58-9880-4bda-994f-9782f73d37b8"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ced512d2e2424e4e8ce6192d47545d3b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00\u003c?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["aqam_cat = AQACCQ_CAT().to(cuda_dev)\n","\n","train_cat_a23, test_cat_a23, aqam_cat = train_and_evaluate(\n","    aqam_cat,\n","    binary_acc,\n","    A23_mp_train, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"markdown","metadata":{"id":"Z0hz8V_afKCF"},"source":["## AQA-CCQ-SUM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"d62S9olpfKCN","outputId":"b3bebb9e-f4d5-4635-ce0e-d1908a64cc03"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0465e97935534839819ac4f1e01f9c59","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00\u003c?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["aqam_sum = AQACCQ_SUM().to(cuda_dev)\n","\n","train_sum_a23, test_sum_a23, aqam_sum = train_and_evaluate(\n","    aqam_sum,\n","    binary_acc,\n","    A23_mp_train, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"markdown","metadata":{"id":"aQLTKkeofKTj"},"source":["## AQA-CCQ-AVE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":106},"id":"LI2YlURQfKTo"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc62648fea4e447683037f07a11982cb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00\u003c?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["aqam_ave = AQACCQ_AVE().to(cuda_dev)\n","\n","train_ave_a23, test_ave_a23, aqam_ave = train_and_evaluate(\n","    aqam_ave,\n","    binary_acc,\n","    A23_mp_train, \n","    test_batches,\n","    EPOCHS,\n","    cuda_dev)"]},{"cell_type":"markdown","metadata":{"id":"LYPWPFLrKgF2"},"source":["# Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ut5v_dXQGJMv"},"outputs":[],"source":["results_object = [\n","  [train_cat_a23, test_cat_a23, \"cat\"],\n","  [train_sum_a23, test_sum_a23, \"sum\"],\n","  [train_ave_a23, test_ave_a23, \"ave\"]\n","]\n","\n","results_obj_fn = f\"{RESULTS_NOTE}_raw_results_obj.p\"\n","with open(results_obj_fn, 'wb') as f:\n","  pickle.dump(results_object, f)\n","\n","upload_file_to_gcs(results_obj_fn, f\"{RESULTS_GCS_DIR}/{results_obj_fn}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHwVNkpgpqd_"},"outputs":[],"source":["for train_accs, [test_acc, _, _], label in results_object:\n","  print(f\"{label} | {test_acc:0.4f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNQ2fSd+pJKyyrzZWchjbiv","collapsed_sections":["E2UNaoi3uMlP","AbeAB8Hokb-2","0fSMAjHpkb-4","FC4VN9OX7ASA","ybu8iEZskb-3","glKP3nrhkb-6","YEniTxLUPL-K","CnejujZg8M8q","kNQBRgQkkb_B","noOpZRP8yXpr","Ee2A1EDh0Zgd","7EFQMMop0fyz","5SBoofED-TbS","rPCF7MZNzOAT","TbrkrVp9unHC","h6ilu9ax8bze","4F9mZnh3xKZF","2kxrcLbaCRmg","ynJF50QkecOE","Z0hz8V_afKCF"],"machine_shape":"hm","name":"","toc_visible":true,"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}